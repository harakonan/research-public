Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@inproceedings{Amershi2015,
abstract = {Model building in machine learning is an iterative process. The performance analysis and debugging step typically involves a disruptive cognitive switch from model building to error analysis, discouraging an informed approach to model building. We present ModelTracker, an interactive visualization that subsumes information contained in numerous traditional summary statistics and graphs while displaying example-level performance and enabling direct error examination and debugging. Usage analysis from machine learning practitioners building real models with ModelTracker over six months shows ModelTracker is used often and throughout model building. A controlled experiment focusing on ModelTracker's debugging capabilities shows participants prefer ModelTracker over traditional tools without a loss in model performance.},
address = {New York, New York, USA},
author = {Amershi, Saleema and Chickering, Max and Drucker, Steven M. and Lee, Bongshin and Simard, Patrice and Suh, Jina},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems - CHI '15},
doi = {10.1145/2702123.2702509},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Others/Amershi{\_}et{\_}al{\_}2015{\_}CHI{\_}ModelTracker- Redesigning Performance Analysis Tools for Machine Learning.pdf:pdf},
isbn = {9781450331456},
pages = {337--346},
publisher = {ACM Press},
title = {{ModelTracker: Redesigning Performance Analysis Tools for Machine Learning}},
year = {2015}
}
@article{Zhu2004,
author = {Zhu, JI and Hastie, Trevor},
doi = {10.1093/biostatistics/kxg046},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Shrinkage/Zhu{\_}Hastie{\_}2004{\_}Biostat{\_}Classification of gene microarrays by penalized logistic regression.pdf:pdf},
issn = {1465-4644},
journal = {Biostatistics},
keywords = {cancer diagnosis,feature selection,logistic regression,microarray,support vector machines},
month = {jul},
number = {3},
pages = {427--443},
title = {{Classification of gene microarrays by penalized logistic regression}},
volume = {5},
year = {2004}
}
@article{Wright2015,
abstract = {We introduce the C++ application and R package ranger. The software is a fast implementation of random forests for high dimensional data. Ensembles of classification, regression and survival trees are supported. We describe the implementation, provide examples, validate the package with a reference implementation, and compare runtime and memory usage with other implementations. The new software proves to scale best with the number of features, samples, trees, and features tried for splitting. Finally, we show that ranger is the fastest and most memory efficient implementation of random forests to analyze data on the scale of a genome-wide association study.},
author = {Wright, Marvin N. and Ziegler, Andreas},
doi = {10.18637/jss.v077.i01},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Tree/Wright{\_}Ziegler{\_}2017{\_}JStatSoft{\_}A Fast Implementation of Random Forests for High  Dimensional Data in C++ and R.pdf:pdf},
isbn = {1548-7660},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {c,classification,machine learning,r,random forests,rcpp,recursive parti-,survival analysis,tioning},
number = {1},
title = {{ranger : A Fast Implementation of Random Forests for High Dimensional Data in C++ and R}},
volume = {77},
year = {2017}
}
@misc{WHOa,
author = {WHO},
title = {{WHO - International Classification of Diseases}},
url = {http://www.who.int/classifications/icd/en/},
urldate = {2018-10-10}
}
@article{Quam1993,
abstract = {In this study, a method was developed to identify health plan members with hypertension from insurance claims, using medical records and a patient survey for validation. A sample of 2,079 patients from two study sites with medical service or pharmacy claims indicating a diagnosis of essential hypertension were surveyed, and the medical records of 182 of the 1,275 survey respondents were reviewed. Where the criteria to identify hypertensive patients used both the medical and pharmacy claims, there was 96{\%} agreement with either the medical record or the patient survey. Where the criteria relied on medical claims alone, the agreement rate decreased to 74{\%} with the medical record and 64{\%} with the patient survey. Where the criteria relied on the pharmacy claims alone, the agreement rate was 67{\%} with the medical record and 75{\%} with the patient survey. Combined evidence from medical service and pharmacy claims yielded a high level of agreement with alternative, more costly sources of data in identifying patients with essential hypertension. As it is more thoroughly investigated, claims data should become a more widely accepted resource for epidemiologic research.},
author = {Quam, Lois and Ellis, Lynda B.M. and Venus, Pat and Clouse, Jon and Taylor, Cynthia G. and Leatherman, Sheila},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Quam et al. - 1993 - Using claims data for epidemiologic research. The concordance of claims-based criteria with the medical record and.pdf:pdf},
issn = {0025-7079},
journal = {Medical care},
month = {jun},
number = {6},
pages = {498--507},
pmid = {8501997},
title = {{Using claims data for epidemiologic research. The concordance of claims-based criteria with the medical record and patient survey for identifying a hypertensive population.}},
volume = {31},
year = {1993}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vecotr of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units wich are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpoler methods such as the perceptron-convergence procedure.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Neural{\_}Networks/Rumelhart{\_}Hinton{\_}Williams{\_}1986{\_}Nature{\_}Learning representations by back-propagating errors.pdf:pdf},
isbn = {0262661160},
issn = {0028-0836},
journal = {Nature},
month = {oct},
number = {6088},
pages = {533--536},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems.},
author = {Rosenblatt, F},
doi = {10.1037/h0042519},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/SVM/Rosenblatt{\_}1958{\_}PsychoRev{\_}THE PERCEPTRON- A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION IN THE BRAIN.pdf:pdf},
isbn = {0033-295X},
issn = {1939-1471},
journal = {Psychological Review},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain.}},
volume = {65},
year = {1958}
}
@article{Robin2011,
abstract = {Receiver operating characteristic (ROC) curves are useful tools to evaluate classifiers in biomedical and bioinformatics applications. However, conclusions are often reached through inconsistent use or insufficient statistical analysis. To support researchers in their ROC curves analysis we developed pROC, a package for R and S+ that contains a set of tools displaying, analyzing, smoothing and comparing ROC curves in a user-friendly, object-oriented and flexible interface.},
author = {Robin, Xavier and Turck, Natacha and Hainard, Alexandre and Tiberti, Natalia and Lisacek, Fr{\'{e}}d{\'{e}}rique and Sanchez, Jean-Charles and M{\"{u}}ller, Markus},
doi = {10.1186/1471-2105-12-77},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Robin et al. - 2011 - pROC an open-source package for R and S to analyze and compare ROC curves.pdf:pdf},
isbn = {1471-2105 (Electronic) 1471-2105 (Linking)},
issn = {1471-2105},
journal = {BMC Bioinformatics},
number = {1},
pages = {77},
pmid = {21414208},
title = {{pROC: an open-source package for R and S+ to analyze and compare ROC curves}},
volume = {12},
year = {2011}
}
@article{Hoerl1970,
abstract = {In multiple regression it is shown that parameter estimates based on minimum residual sum of squares have a high probability of being unsatisfactory, if not incorrect, if the prediction vectors are not orthogonal. Proposed is an estimation procedure based on adding small positive quantities to the diagonal of X'X. Introduced is the ridge trace, a method for showing in two dimensions the effects of nonorthogonality. It is then shown how to augment X'X to obtain biased estimates with smaller mean square error.},
author = {Hoerl, Arthur E. and Kennard, Robert W.},
doi = {10.2307/1267351},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Shrinkage/Hoerl{\_}Kennard{\_}1970{\_}Technometrics{\_}Ridge Regression.pdf:pdf},
issn = {00401706},
journal = {Technometrics},
month = {feb},
number = {1},
pages = {55},
title = {{Ridge Regression: Biased Estimation for Nonorthogonal Problems}},
volume = {12},
year = {1970}
}
@misc{Pinto2018,
author = {Pinto, David},
title = {{fastknn: Build Fast k-Nearest Neighbor Classifiers}},
url = {https://github.com/davpinto/fastknn},
year = {2018}
}
@book{Wooldridge2010,
address = {Cambridge},
author = {Wooldridge, Jeffrey M},
edition = {2nd},
publisher = {MIT Press},
title = {{Econometric analysis of cross section and panel data}},
year = {2010}
}
@article{Youden1950,
author = {Youden, W. J.},
doi = {10.1002/1097-0142(1950)3:1<32::AID-CNCR2820030106>3.0.CO;2-3},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ROC/Youden{\_}1950{\_}Cancer{\_}INDEX FOR RATING DIAGNOSTIC TESTS.pdf:pdf},
isbn = {0008-543X (Print)$\backslash$r0008-543X (Linking)},
issn = {0008-543X},
journal = {Cancer},
number = {1},
pages = {32--35},
pmid = {15405679},
title = {{Index for rating diagnostic tests}},
volume = {3},
year = {1950}
}
@book{Hastie1990,
address = {London},
author = {Hastie, Trevor and Tibshirani, Robert},
publisher = {Chapman {\&} Hall/CRC},
title = {{Generalized Additive Models}},
year = {1990}
}
@inproceedings{Shepard1968,
address = {New York, USA},
author = {Shepard, Donald},
booktitle = {Proceedings of the 23rd ACM National Conference},
doi = {10.1145/800186.810616},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Nearest{\_}Neighbors/Shepard{\_}1968{\_}ACM{\_} A two-dimensional interpolation function for irregularly-spaced data.pdf:pdf},
isbn = {1-59593-161-9},
pages = {517--524},
publisher = {ACM},
title = {{A two-dimensional interpolation function for irregularly-spaced data}},
year = {1968}
}
@article{Katz1997,
abstract = {OBJECTIVE To examine the sensitivity and positive predictive value of Medicare physician claims for select rheumatic conditions managed in rheumatology specialty practices. METHODS Eight rheumatologists in 3 states abstracted 378 patient office records to obtain information on diagnosis and office procedures. The Medicare Part B physician claims for these patient visits were obtained from the Health Care Financing Administration. The sensitivity of the claims data for a specific diagnosis was calculated as the proportion of all patients whose office records for a particular visit documented that diagnosis and who also had physician claims for that visit which identified that diagnosis. The positive predictive value was evaluated in a separate sample of 331 patient visits identified in Medicare physician claims. The positive predictive value of the claims data for a specific diagnosis was calculated as the proportion of patients with that diagnosis coded in the claims for a particular visit who also had the diagnosis documented in the medical record for that visit. RESULTS Ninety percent of abstracted office medical records were matched successfully with Medicare physician claims. The sensitivity of the Medicare physician claims was 0.90 (95{\%} confidence interval [CI] 0.85-0.95) for rheumatoid arthritis (RA), 0.85 (95{\%} CI 0.73-0.97) for systemic lupus erythematosus (SLE), and 0.85 (95{\%} CI 0.78-1.0) for aspiration or injection procedures. The sensitivity for osteoarthritis (OA) of the hip or knee was {\textless} or = 0.50 if 5-digit codes specifying anatomic site were required. The sensitivity for fibromyalgia (FM) was 0.48 (95{\%} CI 0.28-0.68). The positive predictive values were at least 0.90 for RA, SLE, and aspiration or injection procedures. Positive predictive values for FM and the 5-digit site-specific codes for OA of the knee were 0.83 (95{\%} CI 0.66-1.0) and 0.88 (95{\%} CI 0.75-1.0), respectively, while the positive predictive value of the 5-digit site-specific codes for OA of the hip was zero (95{\%} CI 0-0.26). The positive predictive value of OA at any site was 0.83 (95{\%} CI 0.76-0.90). CONCLUSION In specialty practice, Medicare physician claims had high sensitivity and positive predictive value for RA, SLE, OA without specification of anatomic site, and injection or aspiration procedures. The claims had lower sensitivity and predictive value for FM and for OA of the hip. The accuracy of Medicare physician claims for other conditions and in the primary care setting requires further investigation.},
author = {Katz, Jeffrey N. and Barrett, Jane and Liang, Matthew H. and Bacon, Anne M. and Kaplan, Herbert and Kieval, Raphael I. and Lindsey, Stephen M. and Roberts, W. Neal and Sheff, Daniel M. and Spencer, Robert T. and Weaver, Arthur L. and Baron, John A.},
doi = {10.1002/art.1780400908},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference/Katz{\_}et{\_}al{\_}1997{\_}AR{\_}SENSITIVITY AND POSITIVE PREDICTIVE VALUE OF MEDICARE PART B PHYSICIAN CLAIMS FOR RHEUMATOLOGIC DIAGNOSES AND PROCEDURES.pdf:pdf},
issn = {00043591},
journal = {Arthritis {\&} Rheumatism},
month = {sep},
number = {9},
pages = {1594--1600},
pmid = {9324013},
title = {{Sensitivity and positive predictive value of medicare part B physician claims for rheumatologic diagnoses and procedures}},
volume = {40},
year = {1997}
}
@article{Nordstrom2007,
abstract = {This paper explores whether theories of completed suicide are applicable to attempted suicide. It is concluded that theories are needed which apply specifically to attempted suicide, and suggestions are made for sociological and psychological theories of attempted suicide.},
author = {Nordstrom, Beth L. and Norman, Heather S. and Dube, Timothy J. and Wilcox, Marsha A. and Walker, Alexander M.},
doi = {10.1002/pds.1337},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML{\_}CBA/Nordstrom{\_}et{\_}al{\_}2007{\_}Pharmacoepidemiology{\_}and{\_}Drug{\_}Safety{\_}Identification of abacavir hypersensitivity reaction in health care claims data.pdf:pdf},
isbn = {1724-4935},
issn = {10538569},
journal = {Pharmacoepidemiology and Drug Safety},
keywords = {Attempted suicide,Completed suicide},
month = {mar},
number = {3},
pages = {289--296},
pmid = {2009646390},
title = {{Identification of abacavir hypersensitivity reaction in health care claims data}},
volume = {16},
year = {2007}
}
@article{Tikhonov1963,
author = {Tikhonov, Andrey N.},
journal = {Soviet Math. Dokl.},
pages = {1035--1038},
title = {{Solution of incorrectly formulated problems and the regularization method}},
volume = {4},
year = {1963}
}
@article{Cheng2014,
author = {Cheng, Ching-Lan and Lee, Cheng-Han and Chen, Po-Sheng and Li, Yi-Heng and Lin, Swu-Jane and Yang, Yea-Huei Kao},
doi = {10.2188/jea.JE20140076},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Cheng et al. - 2014 - Validation of Acute Myocardial Infarction Cases in the National Health Insurance Research Database in Taiwan.pdf:pdf},
issn = {0917-5040},
keywords = {acute myocardial infarction,nhird,pharmacoepidemiology,taiwan,validity},
number = {6},
pages = {500--507},
title = {{Validation of Acute Myocardial Infarction Cases in the National Health Insurance Research Database in Taiwan}},
volume = {24},
year = {2014}
}
@book{Mohri2012,
address = {Cambridge and London},
author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
isbn = {978-0-262-01825-8},
publisher = {MIT Press},
title = {{Foundations of Machine Learning}},
year = {2012}
}
@article{Song2010,
abstract = {BACKGROUND: Current methods of risk adjustment rely on diagnoses recorded in clinical and administrative records. Differences among providers in diagnostic practices could lead to bias.$\backslash$n$\backslash$nMETHODS: We used Medicare claims data from 1999 through 2006 to measure trends in diagnostic practices for Medicare beneficiaries. Regions were grouped into five quintiles according to the intensity of hospital and physician services that beneficiaries in the region received. We compared trends with respect to diagnoses, laboratory testing, imaging, and the assignment of Hierarchical Condition Categories (HCCs) among beneficiaries who moved to regions with a higher or lower intensity of practice.$\backslash$n$\backslash$nRESULTS: Beneficiaries within each quintile who moved during the study period to regions with a higher or lower intensity of practice had similar numbers of diagnoses and similar HCC risk scores (as derived from HCC coding algorithms) before their move. The number of diagnoses and the HCC measures increased as the cohort aged, but they increased to a greater extent among beneficiaries who moved to regions with a higher intensity of practice than among those who moved to regions with the same or lower intensity of practice. For example, among beneficiaries who lived initially in regions in the lowest quintile, there was a greater increase in the average number of diagnoses among those who moved to regions in a higher quintile than among those who moved to regions within the lowest quintile (increase of 100.8{\%}; 95{\%} confidence interval [CI], 89.6 to 112.1; vs. increase of 61.7{\%}; 95{\%} CI, 55.8 to 67.4). Moving to each higher quintile of intensity was associated with an additional 5.9{\%} increase (95{\%} CI, 5.2 to 6.7) in HCC scores, and results were similar with respect to laboratory testing and imaging.$\backslash$n$\backslash$nCONCLUSIONS: Substantial differences in diagnostic practices that are unlikely to be related to patient characteristics are observed across U.S. regions. The use of clinical or claims-based diagnoses in risk adjustment may introduce important biases in comparative-effectiveness studies, public reporting, and payment reforms.},
author = {Song, Yunjie and Skinner, Jonathan and Bynum, Julie and Sutherland, Jason and Wennberg, John E and Fisher, Elliott S},
doi = {10.1056/NEJMsa0910881},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Song et al. - 2010 - Regional variations in diagnostic practices.pdf:pdf},
isbn = {1533-4406 (Electronic)$\backslash$r0028-4793 (Linking)},
issn = {1533-4406},
journal = {The New England journal of medicine},
keywords = {Diagnostic Techniques and Procedures,Diagnostic Techniques and Procedures: trends,Diagnostic Techniques and Procedures: utilization,Female,Humans,Male,Medicare,Medicare: statistics {\&} numerical data,Medicare: utilization,Physicians',Physicians': statistics {\&} numer,Physicians': trends,Population Dynamics,Practice Patterns,Regression Analysis,Residence Characteristics,Risk Adjustment,United States},
number = {1},
pages = {45--53},
pmid = {20463332},
title = {{Regional variations in diagnostic practices.}},
volume = {363},
year = {2010}
}
@article{Breiman2001,
author = {Breiman, Leo},
doi = {10.1023/A:1010933404324},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Tree/Breiman{\_}2001{\_}ML{\_}Random Forests.pdf:pdf},
journal = {Machine Learning},
keywords = {classification,ensemble,regression},
number = {5},
pages = {5--32},
title = {{Random Forests}},
volume = {45},
year = {2001}
}
@article{Donoho2003,
abstract = {Given a dictionary D = {\{}d(k){\}} of vectors d(k), we seek to represent a signal S as a linear combination S = summation operator(k) gamma(k)d(k), with scalar coefficients gamma(k). In particular, we aim for the sparsest representation possible. In general, this requires a combinatorial optimization process. Previous work considered the special case where D is an overcomplete system consisting of exactly two orthobases and has shown that, under a condition of mutual incoherence of the two bases, and assuming that S has a sufficiently sparse representation, this representation is unique and can be found by solving a convex optimization problem: specifically, minimizing the l(1) norm of the coefficients gamma. In this article, we obtain parallel results in a more general setting, where the dictionary D can arise from two or several bases, frames, or even less structured systems. We sketch three applications: separating linear features from planar ones in 3D data, noncooperative multiuser encoding, and identification of over-complete independent component models.},
author = {Donoho, D. L. and Elad, M.},
doi = {10.1073/pnas.0437847100},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Shrinkage/Donoho{\_}Elad{\_}2003{\_}PNAS{\_}Optimally sparse representation in general (nonorthogonal) dictionaries via 􏱺l1 minimization.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {5},
pages = {2197--2202},
pmid = {16576749},
title = {{Optimally sparse representation in general (nonorthogonal) dictionaries via {\$}l{\^{}}1{\$} minimization}},
volume = {100},
year = {2003}
}
@article{Kawasumi2011,
abstract = {OBJECTIVE: To develop and validate the accuracy of a predictive model to identify adult asthmatics from administrative health care databases. STUDY SETTING: An existing electronic medical record project in Montreal, Quebec. STUDY DESIGN: One thousand four hundred and thirty-one patients with confirmed asthma status were identified from primary care physician's electronic medical record. DATA COLLECTION/EXTRACTION METHODS: Therapeutic indication of asthma in an electronic prescription and/or confirmed asthma from an automated problem list were used as the gold standard. Five groups of asthma-specific markers were identified from administrative health care databases to estimate the probability of the presence of asthma. Cross-validation evaluated the diagnostic ability of each predictive model using 50 percent of sample. PRINCIPAL FINDINGS: The best performance in discriminating between the patients with asthma and those without it included indicators from medical service and prescription claims databases. The best-fitting algorithm had a sensitivity of 70 percent, a specificity of 94 percent, and positive predictive value of 65 percent. The prescriptions claims-specific algorithm demonstrated a nearly equal performance to the model with medical services and prescription claims combined. CONCLUSIONS: Our algorithm using asthma-specific markers from administrative claims databases provided moderate sensitivity and high specificity.},
author = {Kawasumi, Yuko and Abrahamowicz, Michal and Ernst, Pierre and Tamblyn, Robyn},
doi = {10.1111/j.1475-6773.2010.01235.x},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Kawasumi et al. - 2011 - Development and validation of a predictive algorithm to identify adult asthmatics from medical services and pha.pdf:pdf},
isbn = {1475-6773 (Electronic)$\backslash$r0017-9124 (Linking)},
issn = {00179124},
journal = {Health Services Research},
keywords = {Asthma/epidemiology,algorithms,computerized,databases,health services,medical record system},
number = {3},
pages = {939--963},
pmid = {21275988},
title = {{Development and validation of a predictive algorithm to identify adult asthmatics from medical services and pharmacy claims databases}},
volume = {46},
year = {2011}
}
@misc{MHLW2012,
author = {{Ministry of Health Labour and Welfare}},
title = {{Survey on State of Employees' Health in 2012 (in Japanese)}},
url = {http://www.e-stat.go.jp/},
urldate = {2018-10-10},
year = {2012}
}
@article{Belloni2011,
abstract = {This article is about estimation and inference methods for high dimensional sparse (HDS) regression models in econometrics. High dimensional sparse models arise in situations where many regressors (or series terms) are available and the regression function is well-approximated by a parsimonious, yet unknown set of regressors. The latter condition makes it possible to estimate the entire regression function effectively by searching for approximately the right set of regressors. We discuss methods for identifying this set of regressors and estimating their coefficients based on {\$}\backslashell{\_}1{\$}-penalization and describe key theoretical results. In order to capture realistic practical situations, we expressly allow for imperfect selection of regressors and study the impact of this imperfect selection on estimation and inference results. We focus the main part of the article on the use of HDS models and methods in the instrumental variables model and the partially linear model. We present a set of novel inference results for these models and illustrate their use with applications to returns to schooling and growth regression.},
archivePrefix = {arXiv},
arxivId = {1201.0220},
author = {Belloni, Alexandre and Chernozhukov, Victor and Hansen, Christian},
eprint = {1201.0220},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/HDS/Belloni{\_}Chernozhukov{\_}Hansen{\_}2012{\_}INFERENCE FOR HIGH-DIMENSIONAL SPARSE ECONOMETRIC MODELS.pdf:pdf},
keywords = {econometrics,growth,high-dimensional,inference under imperfect model,instrumental regression,partially linear regression,returns-to-schooling,selection,structural e ff ects},
month = {dec},
pages = {1--41},
title = {{Inference for High-Dimensional Sparse Econometric Models}},
year = {2011}
}
@book{Cristianini2000,
address = {New York},
author = {Cristianini, Nello and Shawe-Taylor, John},
isbn = {0-521-78019-5},
publisher = {Cambridge University Press},
title = {{An Introduction to Support Vector Machines and Other Kernel-based Learning Methods}},
year = {2000}
}
@book{Mardia1979,
address = {New York},
author = {Mardia, Kanti V. and Kent, John T. and Bibby, John M.},
publisher = {Academic Press},
title = {{Multivariate Analysis}},
year = {1979}
}
@article{Scholes2011,
abstract = {background: Research and surveillance work addressing ectopic pregnancy often rely on diagnosis and procedure codes available from automated data sources. However, the use of these codes may result in misclassification of cases. Our aims were to evaluate the accuracy of standard ectopic pregnancy codes; and, through the use of additional automated data, to develop and validate a classification algorithm that could potentially improve the accuracy of ectopic pregnancy case identification. methods: Using automated databases from two US managed-care plans, Group Health Cooperative (GH) and Kaiser Permanente Col-orado (KPCO), we sampled women aged 15 –44 with an ectopic pregnancy diagnosis or procedure code from 2001 to 2007 and verified their true case status through medical record review. We calculated positive predictive values (PPV) for code-selected cases compared with true cases at both sites. Using additional variables from the automated databases and classification and regression tree (CART) analysis, we developed a case-finding algorithm at GH (n ¼ 280), which was validated at KPCO (n ¼ 500). results: Compared with true cases, the PPV of code-selected cases was 68 and 81{\%} at GH and KPCO, respectively. The case-finding algorithm identified three predictors: ≥2 visits with an ectopic pregnancy code within 180 days; International Classification of Diseases, 9th Revision, Clinical Modification codes for tubal pregnancy; and methotrexate treatment. Relative to true cases, performance measures for the development and validation sets, respectively, were: 93 and 95{\%} sensitivity; 81 and 81{\%} specificity; 91 and 96{\%} PPV; 84 and 79{\%} negative predictive value. Misclassification proportions were 32{\%} in the development set and 19{\%} in the validation set when using standard codes; they were 11 and 8{\%}, respectively, when using the algorithm. conclusions: The ectopic pregnancy algorithm improved case-finding accuracy over use of standard codes alone and generalized well to a second site. When using administrative data to select potential ectopic pregnancy cases, additional widely available automated health plan data offer the potential to improve case identification.},
author = {Scholes, D. and Yu, O. and Raebel, M. A. and Trabert, B. and Holt, V. L.},
doi = {10.1093/humrep/der299},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML{\_}CBA/Scholes{\_}et{\_}al{\_}2011{\_}HumanReproduction{\_}Improving automated case finding for ectopic pregnancy using a classification algorithm.pdf:pdf},
issn = {0268-1161},
journal = {Human Reproduction},
keywords = {algorithms,classification trees,ectopic pregnancy,information systems,sensitivity and specificity},
month = {nov},
number = {11},
pages = {3163--3168},
title = {{Improving automated case finding for ectopic pregnancy using a classification algorithm}},
volume = {26},
year = {2011}
}
@article{Raskutti2011,
author = {Raskutti, Garvesh and Wainwright, Martin J. and Yu, Bin},
doi = {10.1109/TIT.2011.2165799},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/HDS/Raskutti{\_}Wainwright{\_}Yu{\_}2011{\_}IEEE{\_}Minimax rates of estimation for high-dimensional linear regression over lq-balls.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
month = {oct},
number = {10},
pages = {6976--6994},
title = {{Minimax Rates of Estimation for High-Dimensional Linear Regression Over Lq-Balls}},
volume = {57},
year = {2011}
}
@article{Cheng2011,
abstract = {OBJECTIVE The National Health Insurance Research Database (NHIRD) is commonly used for pharmacoepidemiological research in Taiwan. This study evaluated the validity of the database for patients with a principal diagnosis of ischemic stroke. STUDY DESIGN AND METHODS This cross-sectional study compares records in the NHIRD with those in one medical center. Patients hospitalized for ischemic stroke in 1999 were identified from both databases. The discharge notes, laboratory data, and medication orders during admission and the first discharge visit were reviewed to validate ischemic stroke diagnoses and aspirin prescribing in the NHIRD. Agreement between the two databases in comorbidities of ischemic stroke diagnosis was evaluated using ICD-9 codes. RESULTS Three hundred and seventy two cases were identified from the NHIRD; among them, 364 cases (97.85{\%}) were confirmed as ischemic stroke by radiology examination and clinical presentation. Among these confirmed cases, 344 (94.51{\%}) were assigned 'ischemic stroke' as the principal diagnosis in the NHIRD. The overall agreement of comorbid diagnoses between the databases was 48.39{\%}. The PPV for selected conditions also varied widely, from 0.50 for fracture to 1.00 for colon cancer. The accuracy of recorded aspirin prescriptions was higher in first post-discharge visits (PPV = 0.94) than during hospitalization (PPV = 0.88). CONCLUSION The accuracy of the NHIRD in recording ischemic stroke diagnoses and aspirin prescriptions was high, and the NHIRD appears to be a valid resource for population research in ischemic stroke.},
author = {Cheng, Ching-Lan and Kao, Yea-Huei Yang and Lin, Swu-Jane and Lee, Cheng-Han and Lai, Ming Liang},
doi = {10.1002/pds.2087},
issn = {1099-1557},
journal = {Pharmacoepidemiology and drug safety},
month = {mar},
number = {3},
pages = {236--42},
pmid = {21351304},
title = {{Validation of the National Health Insurance Research Database with ischemic stroke cases in Taiwan.}},
volume = {20},
year = {2011}
}
@article{Morgan1963,
author = {Morgan, James N. and Sonquist, John A.},
doi = {10.1080/01621459.1963.10500855},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Tree/Morgan{\_}Sonquist{\_}1963{\_}JASA{\_}Problems in the Analysis of Survey Data and a Proposal.pdf:pdf},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
month = {jun},
number = {302},
pages = {415--434},
title = {{Problems in the Analysis of Survey Data, and a Proposal}},
volume = {58},
year = {1963}
}
@article{Muhajarine1997,
abstract = {Using linked data from the Manitoba (Canada) Heart Health Survey (MHHS) and physician service claims files we assessed the degree to which self-reported hypertension and clinically measured hypertension agreed with physician claims hypertension and examined the likely sources of disagreement. The overall agreement between survey and claims data for hypertension detection was moderate to high: 82{\%} (kappa = 0.56) for self-reported and physician claims hypertension and 85{\%} (kappa = 0.60) for clinically measured and physician claims hypertension. In the comparison between self-report and physician claims, those who were classified as obese, diabetic, or a homemaker were significantly more likely to have a hypertension measure not confirmed by the other. Disagreement between clinically measured and physician claims was also more common among the obese and homemakers, as well as those on medication for heart diseases, elevated cholesterol levels (LDL), and 35 years of age and older. The high overall level of agreement among these three measures suggest that each may be used with confidence as an indication of hypertension; however, the agreement appears lower among individuals presenting a more complicated clinical profile.},
author = {Muhajarine, Nazeem and Mustard, Cameron and Roos, Leslie L. and Young, T. Kue and Gelskey, Dale E.},
doi = {10.1016/S0895-4356(97)00019-X},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Muhajarine et al. - 1997 - Comparison of survey and physician claims data for detecting hypertension.pdf:pdf},
isbn = {0895-4356 (Print)$\backslash$n0895-4356 (Linking)},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Administrative data,Clinical data,Data linkage,Health surveys,Hypertension,Validity},
number = {6},
pages = {711--718},
pmid = {9250269},
title = {{Comparison of survey and physician claims data for detecting hypertension}},
volume = {50},
year = {1997}
}
@book{Hastie2009,
address = {New York},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
doi = {10.1007/978-0-387-84858-7},
edition = {2nd},
isbn = {978-0-387-84858-7},
pages = {745},
publisher = {Springer},
title = {{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}},
year = {2009}
}
@inproceedings{Simard1992,
abstract = {Memory-based classification algorithms such as radial basis functions or K-nearest neighbors typically rely on simple distances (Euclidean, dot product...), which are not particularly meaningful on pattern vectors. More complex, better suited distance measures are often expensive and rather ad-hoc (elastic matching, deformable templates). We propose a new distance measure which (a) can be made locally invariant to any set of transformations of the input and (b) can be computed efficiently. We tested the method on large handwritten character databases provided by the Post Office and the NIST. Using invariances with respect to translation, rotation, scaling, shearing and line thickness, the method consistently outperformed all other systems on the same databases.},
author = {Simard, Patrice and LeCun, Yann and Denker, John S.},
booktitle = {NIPS'92: Proceedings of the 5th International Conference on Neural Information Processing Systems},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Nearest{\_}Neighbors/Simard{\_}Le Cun{\_}Denker{\_}1992{\_}Advances in Neural Information Processing Systems{\_}Efficient Pattern Recognition Using a New Transformation Distance.pdf:pdf},
isbn = {1558602747},
pages = {50--58},
title = {{Efficient pattern recognition using a new transformation distance}},
year = {1992}
}
@article{Paul2008,
abstract = {We consider regression problems where the number of predictors greatly exceeds the number of observations. We propose a method for variable selection that first estimates the regression function, yielding a "pre-conditioned" response variable. The primary method used for this initial regression is supervised principal components. Then we apply a standard procedure such as forward stepwise selection or the LASSO to the pre-conditioned response variable. In a number of simulated and real data examples, this two-step procedure outperforms forward stepwise selection or the usual LASSO (applied directly to the raw outcome). We also show that under a certain Gaussian latent variable model, application of the LASSO to the pre-conditioned response variable is consistent as the number of predictors and observations increases. Moreover, when the observational noise is rather large, the suggested procedure can give a more accurate estimate than LASSO. We illustrate our method on some real problems, including survival analysis with microarray data.},
author = {Paul, Debashis and Bair, Eric and Hastie, Trevor and Tibshirani, Robert},
doi = {10.1214/009053607000000578},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Pre-processing/Paul{\_}et{\_}al{\_}2008{\_}AnnStat{\_}“PRECONDITIONING” FOR FEATURE SELECTION AND REGRESSION IN HIGH-DIMENSIONAL PROBLEMS.pdf:pdf},
isbn = {0090-5364},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Lasso,Model selection,Prediction error},
month = {aug},
number = {4},
pages = {1595--1618},
title = {{“Preconditioning” for feature selection and regression in high-dimensional problems}},
volume = {36},
year = {2008}
}
@article{Hara2018,
author = {Hara, Konan and Tomio, Jun and Svensson, Thomas and Ohkuma, Rika and Svensson, Akiko Kishi and Yamazaki, Tsutomu},
doi = {10.1016/j.jclinepi.2018.03.004},
file = {:Users/harakonan/Dropbox/Research/Published/Hara{\_}et{\_}al{\_}2018{\_}JCE{\_}Association measures of claims-based algorithms for common chronic conditions were assessed using regularly collected data in Japan.pdf:pdf},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {administrative data,algorithms,association measures,diabetes,dyslipidemia,hypertension},
month = {jul},
pages = {84--95},
publisher = {Elsevier Inc},
title = {{Association measures of claims-based algorithms for common chronic conditions were assessed using regularly collected data in Japan}},
volume = {99},
year = {2018}
}
@article{Waldron2011,
author = {Waldron, Levi and Pintilie, Melania and Tsao, Ming-sound and Shepherd, Frances A and Huttenhower, Curtis and Jurisica, Igor},
doi = {10.1093/bioinformatics/btr591},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Shrinkage/Waldron{\_}et{\_}al{\_}2011{\_}Bioinformatics{\_}Optimized application of penalized regression methods to diverse genomic data.pdf:pdf},
issn = {1460-2059},
journal = {Bioinformatics},
month = {dec},
number = {24},
pages = {3399--3406},
title = {{Optimized application of penalized regression methods to diverse genomic data}},
volume = {27},
year = {2011}
}
@book{JapanAtherosclerosisSocietyEds.2013,
address = {Tokyo},
author = {{Japan Atherosclerosis Society (Eds.)}},
publisher = {Japan Atherosclerosis Society},
title = {{Guidelines for the management of Dyslipidemia 2013 (in Japanese)}},
year = {2013}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Neural{\_}Networks/McCulloch{\_}Pitts{\_}1943{\_}BMB{\_}A logical calculus of the ideas immanent in nervous activity.pdf:pdf},
isbn = {0007-4985},
issn = {0007-4985},
journal = {The Bulletin of Mathematical Biophysics},
month = {dec},
number = {4},
pages = {115--133},
title = {{A logical calculus of the ideas immanent in nervous activity}},
volume = {5},
year = {1943}
}
@article{Malley2012,
abstract = {Background: Most machine learning approaches only provide a classification for binary responses. However, probabilities are required for risk estimation using individual patient characteristics. It has been shown recently that every statistical learning machine known to be consistent for a nonparametric regression problem is a probability machine that is provably consistent for this estimation problem.},
author = {Malley, J. D. and Kruppa, J. and Dasgupta, A. and Malley, K. G. and Ziegler, A.},
doi = {10.3414/ME00-01-0052},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Tree/Malley{\_}et{\_}al{\_}2012{\_}MInfMed{\_}Probability Machines.pdf:pdf},
isbn = {1000490262},
issn = {0026-1270},
journal = {Methods of Information in Medicine},
keywords = {Brier score,Consistency,K nearest neighbor,Logistic regression,Probability estimation,Random forest},
month = {jan},
number = {01},
pages = {74--81},
pmid = {21915433},
title = {{Probability Machines}},
volume = {51},
year = {2012}
}
@article{Ostbye2008,
abstract = {To estimate the proportion of seniors with dementia from three independent data sources and their agreement.},
author = {{\O}stbye, Truls and Taylor, Donald H. and Clipp, Elizabeth C. and Scoyoc, Lynn Van and Plassman, Brenda L.},
doi = {10.1111/j.1475-6773.2007.00748.x},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/{\O}stbye et al. - 2008 - Identification of dementia Agreement among national survey data, medicare claims, and death certificates.pdf:pdf},
isbn = {0017-9124 (Print)$\backslash$r0017-9124 (Linking)},
issn = {00179124},
journal = {Health Services Research},
keywords = {Agreement,Cognitive impairment,Death certificates,Dementia,Health surveys,Longitudinal health surveys,Medicare claims data},
number = {1},
pages = {313--326},
pmid = {18211532},
title = {{Identification of dementia: Agreement among national survey data, medicare claims, and death certificates}},
volume = {43},
year = {2008}
}
@article{Rector2004,
abstract = {OBJECTIVE: To examine the effects of varying diagnostic and pharmaceutical criteria on the performance of claims-based algorithms for identifying beneficiaries with hypertension, heart failure, chronic lung disease, arthritis, glaucoma, and diabetes. STUDY SETTING: Secondary 1999-2000 data from two Medicare+Choice health plans. STUDY DESIGN: Retrospective analysis of algorithm specificity and sensitivity. DATA COLLECTION: Physician, facility, and pharmacy claims data were extracted from electronic records for a sample of 3,633 continuously enrolled beneficiaries who responded to an independent survey that included questions about chronic diseases. PRINCIPAL FINDINGS: Compared to an algorithm that required a single medical claim in a one-year period that listed the diagnosis, either requiring that the diagnosis be listed on two separate claims or that the diagnosis to be listed on one claim for a face-to-face encounter with a health care provider significantly increased specificity for the conditions studied by 0.03 to 0.11. Specificity of algorithms was significantly improved by 0.03 to 0.17 when both a medical claim with a diagnosis and a pharmacy claim for a medication commonly used to treat the condition were required. Sensitivity improved significantly by 0.01 to 0.20 when the algorithm relied on a medical claim with a diagnosis or a pharmacy claim, and by 0.05 to 0.17 when two years rather than one year of claims data were analyzed. Algorithms that had specificity more than 0.95 were found for all six conditions. Sensitivity above 0.90 was not achieved all conditions. CONCLUSIONS: Varying claims criteria improved the performance of case-finding algorithms for six chronic conditions. Highly specific, and sometimes sensitive, algorithms for identifying members of health plans with several chronic conditions can be developed using claims data.},
author = {Rector, Thomas S. and Wickstrom, Steven L. and Shah, Mona and Greeenlee, N. Thomas and Rheault, Paula and Rogowski, Jeannette and Freedman, Vicki and Adams, John and Escarce, Jos{\'{e}} J.},
doi = {10.1111/j.1475-6773.2004.00321.x},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Rector et al. - 2004 - Specificity and sensitivity of claims-based algorithms for identifying members of MedicareChoice health plans tha.pdf:pdf},
isbn = {0017-9124 (Print)$\backslash$n0017-9124 (Linking)},
issn = {0017-9124},
journal = {Health Services Research},
keywords = {algorithms,and health policy-,claim review,health plan administrators,health services researchers,medical informatics,specificity and sensitivity},
number = {6},
pages = {1839--1857},
pmid = {15533190},
title = {{Specificity and sensitivity of claims-based algorithms for identifying members of Medicare+Choice health plans that have chronic medical conditions.}},
volume = {39},
year = {2004}
}
@article{Fisher1936,
author = {Fisher, Ronald A.},
doi = {10.1111/j.1469-1809.1936.tb02137.x},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/LDAs/Fisher{\_}1936{\_}AnnEugenics{\_}THE USE OF MULTIPLE MEASUREMENTS IN TAXONOMIC PROBLEMS.pdf:pdf},
journal = {Annals of Eugenics},
pages = {179--188},
title = {{The use of multiple measurements in taxonomic problems}},
volume = {7},
year = {1936}
}
@article{Vapnik1999,
author = {Vapnik, Vladimir N},
doi = {10.1109/72.788640},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Vapnik{\_}1999{\_}IEEE{\_}An Overview of Statistical Learning Theory.pdf:pdf},
journal = {IEEE Transactions on Neural Networks},
keywords = {Algorithm design and analysis,Loss measurement,Machine learning,Multidimensional systems,Pattern recognition,Probability distribution,Risk management,Statistical learning,Support vector machines,estimation theory,function estimation,generalisation (artificial intelligence),generalization conditions,learning (artificial intelligence),multidimensional function estimation,statistical analysis,statistical learning theory,support vector machines},
number = {5},
pages = {988--999},
title = {{An Overview of Statistical Learning Theory}},
volume = {10},
year = {1999}
}
@misc{Helleputte2017,
author = {Helleputte, Thibault},
title = {{LiblineaR: Linear Predictive Models Based on the LIBLINEAR C/C++ Library}},
url = {https://cran.r-project.org/package=LiblineaR},
year = {2017}
}
@book{Venables2002,
address = {New York},
author = {Venables, W. N. and Ripley, B. D.},
edition = {Fourth},
isbn = {0-387-95457-0},
publisher = {Springer},
title = {{Modern Applied Statistics with S}},
year = {2002}
}
@article{Candes2007,
abstract = {In many important statistical applications, the number of variables or parameters p is much larger than the number of observations n. Suppose then that we have observations y = X$\beta$ + z, where $\beta$ ∈ R p is a parameter vector of interest, X is a data matrix with possibly far fewer rows than columns, n p, and the z i 's are i.i.d. N(0, $\sigma$ 2). Is it possible to estimate $\beta$ reliably based on the noisy data y? To estimate $\beta$, we introduce a new estimator—we call it the Dantzig se-lector—which is a solution to the 1 -regularization problem mi{\~{n}} $\beta$∈R $\beta$ 1 subject to X * r ∞ ≤ (1 + t −1) 2 log p {\textperiodcentered} $\sigma$, where r is the residual vector y − $\beta$ and t is a positive scalar. We show that if X obeys a uniform uncertainty principle (with unit-normed columns) and if the true parameter vector $\beta$ is sufficiently sparse (which here roughly guarantees that the model is identifiable), then with very large probability $\beta$ − $\beta$ 2},
author = {Candes, Emmanuel and Tao, Terence},
doi = {10.1214/009053606000001523},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Shrinkage/Candes{\_}Tao{\_}2007{\_}AnnStat{\_}THE DANTZIG SELECTOR- STATISTICAL ESTIMATION WHEN p IS MUCH LARGER THAN n.pdf:pdf},
isbn = {0090-5364},
issn = {0090-5364},
journal = {The Annals of Statistics},
keywords = {Geometry in high dimensions,Ideal estimation,Linear programming,Model selection,Oracle inequalities,Random matrices,Restricted orthonormality,Sparse solutions to underdetermined systems,Statistical linear model,ℓ1- minimization},
month = {dec},
number = {6},
pages = {2313--2351},
pmid = {253077800001},
title = {{The Dantzig selector: Statistical estimation when {\$}p{\$} is much larger than {\$}n{\$}}},
volume = {35},
year = {2007}
}
@book{TheJapaneseSocietyofHypertensionEds.2014,
address = {Tokyo},
author = {{The Japanese Society of Hypertension (Eds.)}},
publisher = {The Japanese Society of Hypertension},
title = {{Guidelines for the management of Hypertension 2014 (in Japanese)}},
year = {2014}
}
@article{Chan2016,
abstract = {IMPORTANCE Keratinocyte carcinoma (nonmelanoma skin cancer) accounts for substantial burden in terms of high incidence and health care costs but is excluded by most cancer registries in North America. Administrative health insurance claims databases offer an opportunity to identify these cancers using diagnosis and procedural codes submitted for reimbursement purposes. OBJECTIVE To apply recursive partitioning to derive and validate a claims-based algorithm for identifying keratinocyte carcinoma with high sensitivity and specificity. DESIGN, SETTING, AND PARTICIPANTS Retrospective study using population-based administrative databases linked to 602 371 pathology episodes from a community laboratory for adults residing in Ontario, Canada, from January 1, 1992, to December 31, 2009. The final analysis was completed in January 2016. We used recursive partitioning (classification trees) to derive an algorithm based on health insurance claims. The performance of the derived algorithm was compared with 5 prespecified algorithms and validated using an independent academic hospital clinic data set of 2082 patients seen in May and June 2011. MAIN OUTCOMES AND MEASURES Sensitivity, specificity, positive predictive value, and negative predictive value using the histopathological diagnosis as the criterion standard. We aimed to achieve maximal specificity, while maintaining greater than 80{\%} sensitivity. RESULTS Among 602 371 pathology episodes, 131 562 (21.8{\%}) had a diagnosis of keratinocyte carcinoma. Our final derived algorithm outperformed the 5 simple prespecified algorithms and performed well in both community and hospital data sets in terms of sensitivity (82.6{\%} and 84.9{\%}, respectively), specificity (93.0{\%} and 99.0{\%}, respectively), positive predictive value (76.7{\%} and 69.2{\%}, respectively), and negative predictive value (95.0{\%} and 99.6{\%}, respectively). Algorithm performance did not vary substantially during the 18-year period. CONCLUSIONS AND RELEVANCE This algorithm offers a reliable mechanism for ascertaining keratinocyte carcinoma for epidemiological research in the absence of cancer registry data. Our findings also demonstrate the value of recursive partitioning in deriving valid claims-based algorithms.},
author = {Chan, An-Wen and Fung, Kinwah and Tran, Jennifer M. and Kitchen, Jessica and Austin, Peter C. and Weinstock, Martin A. and Rochon, Paula A.},
doi = {10.1001/jamadermatol.2016.2609},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML{\_}CBA/Chan{\_}et{\_}al{\_}2016{\_}JAMADerma{\_}Application of Recursive Partitioning to Derive and Validate a Claims-Based Algorithm for Identifying Keratinocyte Carcinoma (Nonmelanoma Skin Cancer).pdf:pdf},
issn = {2168-6068},
journal = {JAMA Dermatology},
month = {oct},
number = {10},
pages = {1122},
pmid = {27533718},
title = {{Application of Recursive Partitioning to Derive and Validate a Claims-Based Algorithm for Identifying Keratinocyte Carcinoma (Nonmelanoma Skin Cancer)}},
volume = {152},
year = {2016}
}
@techreport{BreimanLeo;Ihaka1984,
author = {Breiman, Leo and Ihaka, Ross},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/LDAs/Breiman{\_}Ihaka{\_}1984{\_}TR{\_} Nonlinear Discriminant Analysis Via Scaling and Ace.pdf:pdf},
institution = {University of California, Berkeley},
title = {{Nonlinear Discriminant Analysis Via Scaling and Ace}},
year = {1984}
}
@misc{Hastie2017,
author = {Hastie, Trevor and Tibshirani, Robert and Leisch, Friedrich and Hornik, Kurt and Ripley, Brian D.},
title = {{mda: Mixture and Flexible Discriminant Analysis}},
url = {https://cran.r-project.org/package=mda},
year = {2017}
}
@article{Chawla2002,
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of ``normal'' examples with only a small percentage of ``abnormal'' or ``interesting'' examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (normal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
doi = {10.1613/jair.953},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Pre-processing/Chawla{\_}et{\_}al{\_}2002{\_}JAIR{\_}SMOTE- Synthetic Minority Over-sampling Technique.pdf:pdf},
isbn = {013805326X},
issn = {1076-9757},
journal = {Journal of Artificial Intelligence Research},
month = {jun},
pages = {321--357},
pmid = {18190633},
title = {{SMOTE: Synthetic Minority Over-sampling Technique}},
volume = {16},
year = {2002}
}
@article{Lin2006,
abstract = {We propose a new method for model selection and model fitting in multivariate nonparametric regression models, in the framework of smoothing spline ANOVA. The ``COSSO'' is a method of regularization with the penalty functional being the sum of component norms, instead of the squared norm employed in the traditional smoothing spline method. The COSSO provides a unified framework for several recent proposals for model selection in linear models and smoothing spline ANOVA models. Theoretical properties, such as the existence and the rate of convergence of the COSSO estimator, are studied. In the special case of a tensor product design with periodic functions, a detailed analysis reveals that the COSSO does model selection by applying a novel soft thresholding type operation to the function components. We give an equivalent formulation of the COSSO estimator which leads naturally to an iterative algorithm. We compare the COSSO with MARS, a popular method that builds functional ANOVA models, in simulations and real examples. The COSSO method can be extended to classification problems and we compare its performance with those of a number of machine learning algorithms on real datasets. The COSSO gives very competitive performance in these studies.},
author = {Lin, Yi and Zhang, Hao Helen},
doi = {10.1214/009053606000000722},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/GAM/Lin{\_}Zhang{\_}2006{\_}AnnStat{\_}COMPONENT SELECTION AND SMOOTHING IN MULTIVARIATE NONPARAMETRIC REGRESSION .pdf:pdf},
issn = {00905364},
journal = {Annals of Statistics},
keywords = {Machine learning,Method of regularization,Model selection,Nonparametric classification,Nonparametric regression,Smoothing spline ANOVA},
number = {5},
pages = {2272--2297},
title = {{Component selection and smoothing in multivariate nonparametric regression}},
volume = {34},
year = {2006}
}
@article{Kern2006,
abstract = {OBJECTIVE: To determine prevalence of chronic kidney disease (CKD) in patients with diabetes, and accuracy of International Classification of Diseases, 9th Revision, Clinical Modification (ICD-9-CM) codes to identify such patients. DATA SOURCES/STUDY SETTING: Secondary data from 1999 to 2000. We linked all inpatient and outpatient administrative and clinical records of U.S. veterans with diabetes dually enrolled in Medicare and the Veterans Administration (VA) health care systems. STUDY DESIGN: We used a cross-sectional, observational design to determine the sensitivity and specificity of renal-related ICD-9-CM diagnosis codes in identifying individuals with chronic kidney disease. DATA COLLECTION/EXTRACTION METHODS: We estimated glomerular filtration rate (eGFR) from serum creatinine and defined CKD as Stage 3, 4, or 5 CKD by eGFR criterion according to the Kidney Disease Outcomes Quality Initiative guidelines. Renal-related ICD-9-CM codes were grouped by algorithm. PRINCIPAL FINDINGS: Prevalence of CKD was 31.6 percent in the veteran sample with diabetes. Depending on the detail of the algorithm, only 20.2 to 42.4 percent of individuals with CKD received a renal-related diagnosis code in either VA or Medicare records over 1 year. Specificity of renal codes for CKD ranged from 93.2 to 99.4 percent. Patients hospitalized in VA facilities were slightly more likely to be correctly coded for CKD than patients hospitalized in facilities reimbursed by Medicare (OR 5.4 versus 4.1, p=.0330) CONCLUSIONS: CKD is a common comorbidity for patients with diabetes in the VA system. Diagnosis codes in administrative records from Medicare and VA systems are insensitive, but specific markers for patients with CKD.},
author = {Kern, Elizabeth F. O. and Maney, Miriam and Miller, Donald R. and Tseng, Chin-Lin and Tiwari, Anjali and Rajan, Mangala and Aron, David and Pogach, Leonard},
doi = {10.1111/j.1475-6773.2005.00482.x},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Kern et al. - 2006 - Failure of ICD-9-CM codes to identify patients with comorbid chronic kidney disease in diabetes.pdf:pdf},
isbn = {0017-9124},
issn = {0017-9124},
journal = {Health Services Research},
keywords = {administra-,chronic kidney disease,diabetes,diabetic nephropathy,icd-9-cm codes,tive records},
number = {2},
pages = {564--580},
pmid = {16584465},
title = {{Failure of ICD-9-CM codes to identify patients with comorbid chronic kidney disease in diabetes.}},
volume = {41},
year = {2006}
}
@inproceedings{Boser1992,
address = {New York},
author = {Boser, Bernhard E. and Guyon, Isabelle M and Vapnik, Vladimir N.},
booktitle = {Proceedings of the fifth annual workshop on Computational learning theory},
doi = {10.1145/130385.130401},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/SVM/Boser{\_}Guyon{\_}Vapnik{\_}1992{\_}ACM{\_}A Training Optimal Algorithm for Margin Classifiers.pdf:pdf},
isbn = {089791497X},
issn = {0-89791-497-X},
pages = {144--152},
pmid = {25246403},
publisher = {ACM Press},
title = {{A training algorithm for optimal margin classifiers}},
year = {1992}
}
@book{Breiman1984,
address = {London},
author = {Breiman, Leo and Friedman, Jerome H. and Olshen, Richard A. and Stone, Charles J.},
publisher = {Chapman {\&} Hall/CRC},
title = {{Classification and regression trees}},
year = {1984}
}
@article{McWilliams2016,
abstract = {BackgroundIn the Medicare Shared Savings Program (MSSP), accountable care organizations (ACOs) have financial incentives to lower spending and improve quality. We used quasi-experimental methods to assess the early performance of MSSP ACOs. MethodsUsing Medicare claims from 2009 through 2013 and a difference-in-differences design, we compared changes in spending and in performance on quality measures from before the start of ACO contracts to after the start of the contracts between beneficiaries served by the 220 ACOs entering the MSSP in mid-2012 (2012 ACO cohort) or January 2013 (2013 ACO cohort) and those served by non-ACO providers (control group), with adjustment for geographic area and beneficiary characteristics. We analyzed the 2012 and 2013 ACO cohorts separately because entry time could reflect the capacity of an ACO to achieve savings. We compared ACO savings according to organizational structure, baseline spending, and concurrent ACO contracting with commercial insurers. ResultsAdjusted Medica...},
author = {McWilliams, J. Michael and Hatfield, Laura A. and Chernew, Michael E. and Landon, Bruce E. and Schwartz, Aaron L.},
doi = {10.1056/NEJMsa1600142},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/McWilliams et al. - 2016 - Early Performance of Accountable Care Organizations in Medicare.pdf:pdf},
isbn = {9930499318},
issn = {0028-4793},
journal = {New England Journal of Medicine},
number = {24},
pages = {2357--2366},
pmid = {27075832},
title = {{Early Performance of Accountable Care Organizations in Medicare}},
volume = {374},
year = {2016}
}
@article{VanWalraven2011,
abstract = {Objective: Administrative database research (ADR) frequently uses codes to identify diagnoses or procedures. The association of these codes with the condition it represents must be measured to gauge misclassification in the study. Measure the proportion of ADR studies using diagnostic or procedural codes that measured or referenced code accuracy. Study Design and Setting: Random sample of 150 MEDLINE-cited ADR studies stratified by year of publication. The proportion of ADR studies using codes to define patient cohorts, exposures, or outcomes that measured or referenced code accuracy and Bayesian estimates for probability of disease given code operating characteristics were measured. Results: One hundred fifteen ADR studies (76.7{\%} [95{\%} confidence interval (CI), 69.3-82.8]) used codes. Of these studies, only 14 (12.1{\%} [7.3-19.5]) measured or referenced the association of the code with the entity it supposedly represented. This proportion did not vary by year of publication but was significantly higher in journals with greater impact factors. Of five studies reporting code sensitivity and specificity, the estimated probability of code-related condition in code-positive patients was less than 50{\%} in two. Conclusion: In ADR, diagnostic and procedural codes are commonly used but infrequently validated. People with a code frequently do not have the condition it represents. ?? 2011 Elsevier Inc. All rights reserved.},
author = {{Van Walraven}, Carl and Bennett, Carol and Forster, Alan J.},
doi = {10.1016/j.jclinepi.2011.01.001},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Van Walraven, Bennett, Forster - 2011 - Administrative database research infrequently used validated diagnostic or procedural codes.pdf:pdf},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Administrative database research,Bayesian calculations,Bias,Codes,Misclassification,Systematic review},
number = {10},
pages = {1054--1059},
pmid = {21474278},
publisher = {Elsevier Inc},
title = {{Administrative database research infrequently used validated diagnostic or procedural codes}},
volume = {64},
year = {2011}
}
@article{Donoho2006,
author = {Donoho, David L},
doi = {10.1002/cpa.20131},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Shrinkage/Donoho{\_}2006{\_}CPAM{\_}For Most Large Underdetermined Systems of Equations, the Minimal l1-norm Near-Solution Approximates the Sparsest Near-Solution.pdf:pdf},
isbn = {9781118695692},
issn = {0010-3640},
journal = {Communications on Pure and Applied Mathematics},
keywords = {almost-spherical sections of banach,and phrases,approximate,eigenvalues,equations,solution of underdetermined linear,spaces,sparse solution of linear,systems},
month = {jul},
number = {7},
pages = {907--934},
title = {{For most large underdetermined systems of equations, the minimal {\$}l{\_}1{\$}-norm near-solution approximates the sparsest near-solution}},
volume = {59},
year = {2006}
}
@book{RCoreTeam2018,
address = {Vienna, Austria},
author = {{R Core Team}},
publisher = {R Foundation for Statistical Computing},
title = {{R: A language and environment for statistical computing}},
url = {https://www.r-project.org/},
year = {2018}
}
@article{Klabunde2006,
abstract = {BACKGROUND Identifying appropriate comorbidity data sources is a key consideration in health services and outcomes research. OBJECTIVE Using cancer patients as an example, we compared comorbid conditions identified: 1) on the discharge facesheet versus full hospital medical record and 2) in the hospital record versus Medicare claims, both precancer diagnosis and associated with a cancer treatment-related index hospitalization. METHODS We used data from 1995 Surveillance, Epidemiology and End Results patterns of care studies for 1,382 patients. Comorbid conditions were ascertained from the hospital record associated with the most definitive cancer treatment and Medicare claims. We calculated the prevalence for and assessed concordances among 12 conditions derived from the hospital record facesheet; full hospital record; Medicare claims precancer diagnosis, with and without a rule-out algorithm applied; and Medicare claims associated with an index hospitalization. RESULTS The proportion of patients with one or more comorbid conditions varied by data source, from 21{\%} for the facesheet to 85{\%} for prediagnosis Medicare claims without the rule-out algorithm. Condition prevalences were substantially lower for the facesheet compared with the full hospital record. For prediagnosis Medicare claims, condition prevalences were more than 1.7 times greater in the absence of an algorithm to screen for rule-out diagnoses. Measures assessing concordance between the full hospital record and prediagnosis Medicare claims (rule-out algorithm applied) showed modest agreement. CONCLUSIONS The hospital record and Medicare claims are complementary data sources for identifying comorbid conditions. Comorbidity is greatly underascertained when derived only from the facesheet of the hospital record. Investigators using Part B Medicare claims to measure comorbidity should remove conditions that are listed for purposes of generating bills but are not true comorbidities.},
author = {Klabunde, Carrie N. and Harlan, Linda C. and Warren, Joan L.},
doi = {10.1097/01.mlr.0000223480.52713.b9},
issn = {0025-7079},
journal = {Medical care},
month = {oct},
number = {10},
pages = {921--8},
pmid = {17001263},
title = {{Data sources for measuring comorbidity: a comparison of hospital records and medicare claims for cancer patients.}},
volume = {44},
year = {2006}
}
@article{Breiman1996,
abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
author = {Breiman, Leo},
doi = {10.1023/A:1018054314350},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Breiman{\_}1996{\_}ML{\_}Bagging Predictors.pdf:pdf},
isbn = {0885-6125},
issn = {1573-0565},
journal = {Machine Learning},
keywords = {aggregation,averaging,bootstrap,combining},
pages = {123--140},
title = {{Bagging Predictors}},
volume = {24},
year = {1996}
}
@article{Ravikumar2009,
abstract = {We present a new class of methods for high-dimensional non-parametric regression and classification called sparse additive models (SpAM). Our methods combine ideas from sparse linear modeling and additive nonparametric regression. We derive an algorithm for fitting the models that is practical and effective even when the number of covariates is larger than the sample size. SpAM is essentially a functional version of the grouped lasso of Yuan and Lin (2006). SpAM is also closely related to the COSSO model of Lin and Zhang (2006), but decouples smoothing and sparsity, enabling the use of arbitrary nonparametric smoothers. We give an analysis of the theoretical properties of sparse additive models, and present empirical results on synthetic and real data, showing that SpAM can be effective in fitting sparse nonparametric models in high dimensional data.},
author = {Ravikumar, Pradeep and Lafferty, John and Liu, Han and Wasserman, Larry},
doi = {10.1111/j.1467-9868.2009.00718.x},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/GAM/Ravikumar{\_}et{\_}al{\_}2009{\_}JRSSB{\_}Sparse additive models.pdf:pdf},
issn = {13697412},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {additive models,lasso,non-parametric regression,sparsity},
month = {nov},
number = {5},
pages = {1009--1030},
title = {{Sparse additive models}},
volume = {71},
year = {2009}
}
@article{Nuti2016,
abstract = {IMPORTANCE: Little contemporary information is available about comparative performance between Veterans Affairs (VA) and non-VA hospitals, particularly related to mortality and readmission rates, 2 important outcomes of care.$\backslash$n$\backslash$nOBJECTIVE: To assess and compare mortality and readmission rates among men in VA and non-VA hospitals.$\backslash$n$\backslash$nDESIGN, SETTING, AND PARTICIPANTS: Cross-sectional analysis involving male Medicare fee-for-service beneficiaries aged 65 years or older hospitalized between 2010 and 2013 in VA and non-VA acute care hospitals for acute myocardial infarction (AMI), heart failure (HF), or pneumonia using the Medicare Standard Analytic Files and Enrollment Database together with VA administrative claims data. To avoid confounding geographic effects with health care system effects, we studied VA and non-VA hospitals within the same metropolitan statistical area (MSA).$\backslash$n$\backslash$nEXPOSURES: Hospitalization in a VA or non-VA hospital in MSAs that contained at least 1 VA and non-VA hospital.$\backslash$n$\backslash$nMAIN OUTCOMES AND MEASURES: For each condition, 30-day risk-standardized mortality rates and risk-standardized readmission rates for VA and non-VA hospitals. Mean aggregated within-MSA differences in mortality and readmission rates were also assessed.$\backslash$n$\backslash$nRESULTS: We studied 104 VA and 1513 non-VA hospitals, with each condition-outcome analysis cohort for VA and non-VA hospitals containing at least 7900 patients (men; ≥65 years), in 92 MSAs. Mortality rates were lower in VA hospitals than non-VA hospitals for AMI (13.5{\%} vs 13.7{\%}, P = .02; -0.2 percentage-point difference) and HF (11.4{\%} vs 11.9{\%}, P = .008; -0.5 percentage-point difference), but higher for pneumonia (12.6{\%} vs 12.2{\%}, P = .045; 0.4 percentage-point difference). In contrast, readmission rates were higher in VA hospitals for all 3 conditions (AMI, 17.8{\%} vs 17.2{\%}, 0.6 percentage-point difference; HF, 24.7{\%} vs 23.5{\%}, 1.2 percentage-point difference; pneumonia, 19.4{\%} vs 18.7{\%}, 0.7 percentage-point difference, all P {\textless} .001). In within-MSA comparisons, VA hospitals had lower mortality rates for AMI (percentage-point difference, -0.22; 95{\%} CI, -0.40 to -0.04) and HF (-0.63; 95{\%} CI, -0.95 to -0.31), and mortality rates for pneumonia were not significantly different (-0.03; 95{\%} CI, -0.46 to 0.40); however, VA hospitals had higher readmission rates for AMI (0.62; 95{\%} CI, 0.48 to 0.75), HF (0.97; 95{\%} CI, 0.59 to 1.34), or pneumonia (0.66; 95{\%} CI, 0.41 to 0.91).$\backslash$n$\backslash$nCONCLUSIONS AND RELEVANCE: Among older men with AMI, HF, or pneumonia, hospitalization at VA hospitals, compared with hospitalization at non-VA hospitals, was associated with lower 30-day risk-standardized all-cause mortality rates for AMI and HF, and higher 30-day risk-standardized all-cause readmission rates for all 3 conditions, both nationally and within similar geographic areas, although absolute differences between these outcomes at VA and non-VA hospitals were small.},
author = {Nuti, Sudhakar V. and Qin, Li and Rumsfeld, John S. and Ross, Joseph S. and Masoudi, Frederick A. and Normand, Sharon-Lise T. and Murugiah, Karthik and Bernheim, Susannah M. and Suter, Lisa G. and Krumholz, Harlan M.},
doi = {10.1001/jama.2016.0278},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Nuti et al. - 2016 - Association of Admission to Veterans Affairs Hospitals vs Non-Veterans Affairs Hospitals With Mortality and Readmis.pdf:pdf},
issn = {1538-3598},
journal = {JAMA},
keywords = {80 and over,Aged,Cross-Sectional Studies,Heart Failure,Heart Failure: mortality,Hospital Mortality,Hospitals,Hospitals: statistics {\&} numerical data,Humans,Male,Myocardial Infarction,Myocardial Infarction: mortality,Patient Readmission,Pneumonia,Pneumonia: mortality,United States,Veterans,Veterans: statistics {\&} numerical data},
number = {6},
pages = {582--92},
pmid = {26864412},
title = {{Association of Admission to Veterans Affairs Hospitals vs Non-Veterans Affairs Hospitals With Mortality and Readmission Rates Among Older Men Hospitalized With Acute Myocardial Infarction, Heart Failure, or Pneumonia.}},
volume = {315},
year = {2016}
}
@article{Belloni2010,
abstract = {In this paper we study post-model selection estimators which apply ordinary least squares (ols) to the model selected by first-step penalized estimators, typically lasso. It is well known that lasso can estimate the non-parametric regression function at nearly the oracle rate, and is thus hard to improve upon. We show that ols post lasso estimator performs at least as well as lasso in terms of the rate of convergence, and has the advantage of a smaller bias. Remarkably, this performance occurs even if the lasso-based model selection “fails” in the sense of missing some components of the “true” regression model. By the “true” model we mean here the best s-dimensional approximation to the nonparametric regression function chosen by the oracle. Furthermore, ols post lasso estimator can perform strictly better than lasso, in the sense of a strictly faster rate of convergence, if the lasso-based model selection correctly includes all components of the “true” model as a subset and also achieves sufficient sparsity. In the extreme case, when lasso perfectly selects the “true” model, the ols post lasso estimator becomes the oracle estimator. An important ingredient in our analysis is a new sparsity bound on the dimension of the model selected by lasso which guarantees that this dimension is at most of the same order as the dimension of the “true” model. Our rate results are non-asymptotic and hold in both parametric and nonparametric models. Moreover, our analysis is not limited to the lasso estimator acting as selector in the first step, but also applies to any other estimator, for example various forms of thresholded lasso, with good rates and good sparsity properties. Our analysis covers both traditional thresholding and a new practical, data-driven thresholding scheme that induces maximal sparsity subject to maintaining a certain goodness-of-fit. The latter scheme has theoretical guarantees similar to those of lasso or ols post lasso, but it dominates these procedures as well as traditional thresholding in a wide variety of experiments. First arXiv version: December 2009.},
author = {Belloni, Alexandre and Chernozhukov, Victor},
doi = {10.3150/11-BEJ410},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/HDS/Belloni{\_}Chernozhukov{\_}2013{\_}Bernoulli{\_}Least squares after model selection in high-dimensional sparse models.pdf:pdf},
issn = {1350-7265},
journal = {Bernoulli},
keywords = {H12,J07,J99,lasso,ols post lasso,post-model-selection estimators},
month = {may},
number = {2},
pages = {521--547},
pmid = {10694730},
title = {{Least squares after model selection in high-dimensional sparse models}},
volume = {19},
year = {2013}
}
@article{Friedman2001,
author = {Friedman, Jerome H.},
doi = {10.1214/aos/1013203451},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Tree/Friedman{\_}2001{\_}AnnStat{\_}GREEDY FUNCTION APPROXIMATION- A GRADIENT BOOSTING MACHINE.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {oct},
number = {5},
pages = {1189--1232},
title = {{Greedy function approximation: A gradient boosting machine}},
volume = {29},
year = {2001}
}
@article{Lecun1998,
author = {LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
doi = {10.1109/5.726791},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Neural{\_}Networks/LeCun{\_}et{\_}al{\_}1998{\_}IEEE{\_}Gradient-Based Learning Applied to Document Recognition.pdf:pdf},
issn = {00189219},
journal = {Proceedings of the IEEE},
number = {11},
pages = {2278--2324},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Andrade2002,
abstract = {The automated health plan data and data from medical chart abstractions from eight large health maintenance organizations were used to evaluate the positive predictive values (PPVs) of the International Classification of Diseases, 9th revision (ICD-9) codes for cases of peptic ulcers and upper gastrointestinal bleeding. Overall, 207 of 884 cases of peptic ulcers and upper gastrointestinal bleeding (23{\%}) were confirmed by surgery, endoscopy, X-ray, or autopsy. The PPVs were 66{\%} for hospitalizations with codes for duodenal ulcer (ICD-9-CM 532), 61{\%} for gastric/gastrojejunal ulcer (ICD-9-CM 531, 534), 1{\%} for peptic ulcer (ICD-9-CM 533), and 9{\%} for gastrointestinal hemorrhage (ICD-9-CM578). The overall and diagnostic category-specific PPVs were generally similar for the various HMOs. This study, using data from a large number of health plans located in different geographical regions, underscores the importance of evaluating the accuracy of the diagnoses from automated health plan databases. ?? 2002 Elsevier Science Inc. All rights reserved.},
author = {Andrade, Susan E. and Gurwitz, Jerry H. and Chan, K. Arnold and Donahue, James G. and Beck, Arne and Boles, Myde and Buist, Diana S M and Goodman, Michael and LaCroix, Andrea Z. and Levin, T. R. and Platt, Richard},
doi = {10.1016/S0895-4356(01)00480-2},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Andrade et al. - 2002 - Validation of diagnoses of peptic ulcers and bleeding from administrative databases A multi-health maintenance o.pdf:pdf},
isbn = {0895-4356 (Print) 0895-4356},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Automated databases,Peptic ulcer,Positive predictive value},
number = {3},
pages = {310--313},
pmid = {11864803},
title = {{Validation of diagnoses of peptic ulcers and bleeding from administrative databases: A multi-health maintenance organization study}},
volume = {55},
year = {2002}
}
@article{Losina2003,
abstract = {This analysis was performed to examine whether Medicare claims accurately document underlying rheumatologic diagnoses in total hip replacement (THR) recipients. We obtained data on rheumatologic diagnoses including rheumatoid arthritis (RA), avascular necrosis (AVN), and osteoarthritis (OA) from medical records and from Medicare claims data. To examine the accuracy of claims data we calculated sensitivity and positive predictive value using medical records data as the "gold standard" and assessed bias due to misclassification of claims-based diagnoses. The sensitivities of claims-based diagnoses of RA, AVN, and OA were 0.65, 0.54, and 0.96, respectively; the positive predictive values were all in the 0.86-0.89 range. The sensitivities of RA and AVN varied substantially across hospital volume strata, but in different directions for the two diagnoses. We conclude that inaccuracies in claims coding of diagnoses are frequent, and are potential sources of bias. More studies are needed to examine the magnitude and direction of bias in health outcomes research due to inaccuracy of claims coding for specific diagnoses. ?? 2003 Elsevier Inc. All rights reserved.},
author = {Losina, Elena and Barrett, Jane and Baron, John A. and Katz, Jeffrey N.},
doi = {10.1016/S0895-4356(03)00056-8},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Losina et al. - 2003 - Accuracy of Medicare claims data for rheumatologic diagnoses in total hip replacement recipients.pdf:pdf},
isbn = {0895-4356 (Print)$\backslash$r0895-4356 (Linking)},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Administrative data,Health outcomes research,Hospital procedure volume,Misclassification bias,Sensitivity,Total hip replacement},
number = {6},
pages = {515--519},
pmid = {12873645},
title = {{Accuracy of Medicare claims data for rheumatologic diagnoses in total hip replacement recipients}},
volume = {56},
year = {2003}
}
@article{Iizuka2012,
abstract = {I examine physician agency in health care services in the context of the choice between brand-name and generic pharmaceuticals. $\backslash$nI examine micro-panel data from Japan, where physicians can legally make profits by prescribing and dispensing drugs. The results indicate that physicians often fail to internalize patient costs, explaining why cheaper generics are infrequently adopted. Doctors respond to markup differentials between the two versions, indicating another agency problem. However, generics' markup advantages are short-lived, which limits their impact on increasing generic adoption. Additionally, state dependence and heterogeneous doctor preferences affected generics' adoption. Policy makers can target these factors to improve static efficiency. (JEL D82, I11, J44, L65)},
author = {Iizuka, Toshiaki},
doi = {10.1257/aer.102.6.2826},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Iizuka - 2012 - Physician agency and adoption of generic pharmaceuticals.pdf:pdf},
isbn = {00028282},
issn = {00028282},
journal = {American Economic Review},
number = {6},
pages = {2826--2858},
title = {{Physician agency and adoption of generic pharmaceuticals}},
volume = {102},
year = {2012}
}
@inproceedings{Chen2009,
author = {Chen, Wei and Liu, Tie-Yan and Lan, Yanyan and Ma, Zhiming and Li, Hang},
booktitle = {NIPS '07: Proceedings of the 22nd International Conference on Neural Information Processing Systems},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ROC/Chen{\_}et{\_}al{\_}2009{\_}NIPS{\_}Ranking Measures and Loss Functions in Learning to Rank.pdf:pdf},
month = {mar},
pages = {315--323},
title = {{Ranking Measures and Loss Functions in Learning to Rank}},
year = {2009}
}
@article{Gorina2011,
abstract = {OBJECTIVE: To examine the strengths and limitations of the Center for Medicare and Medicaid Services' Chronic Condition Data Warehouse (CCW) algorithm for identifying chronic conditions in older persons from Medicare beneficiary data. DATA SOURCES: Records from participants of the NHANES I Epidemiologic Follow-up Study (NHEFS 1971-1992) linked to Medicare claims data from 1991 to 2000. STUDY DESIGN: We estimated the percent of preexisting cases of chronic conditions correctly identified by the CCW algorithm during its reference period and the number of years of claims data necessary to find a preexisting condition. PRINCIPAL FINDINGS: The CCW algorithm identified 69 percent of preexisting diabetes cases but only 17 percent of preexisting arthritis cases. Cases identified by the CCW are a mix of preexisting and newly diagnosed conditions. CONCLUSIONS: The prevalence of conditions needing less frequent health care utilization (e.g., arthritis) may be underestimated by the CCW algorithm. The CCW reference periods may not be sufficient for all analytic purposes.},
author = {Gorina, Yelena and Kramarow, Ellen A.},
doi = {10.1111/j.1475-6773.2011.01277.x},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Gorina, Kramarow - 2011 - Identifying chronic conditions in medicare claims data Evaluating the chronic condition data warehouse algorit.pdf:pdf},
isbn = {1475-6773 (Electronic)$\backslash$r0017-9124 (Linking)},
issn = {00179124},
journal = {Health Services Research},
keywords = {CCW,Medicare claims,NHEFS,chronic conditions},
number = {5},
pages = {1610--1627},
pmid = {21649659},
title = {{Identifying chronic conditions in medicare claims data: Evaluating the chronic condition data warehouse algorithm}},
volume = {46},
year = {2011}
}
@book{Collet1999,
address = {Boca Raton, Florida},
author = {Collet, David},
edition = {Second},
publisher = {Chapman {\&} Hall/CRC},
title = {{Modelling Binary Data}},
year = {1999}
}
@article{Yamana2016,
abstract = {BACKGROUND: Diagnoses recorded in administrative databases have limited utility for accurate identification of severe sepsis and disseminated intravascular coagulation (DIC). We evaluated the performance of alternative identification methods that use procedure records. METHODS: We obtained data for adult patients admitted to intensive care units in three hospitals during a 1-year period. Severe sepsis and DIC were identified by three means: laboratory data, diagnoses, and procedures. Using laboratory data as a reference, the sensitivity and specificity of procedure-based methods and diagnosis-based methods were compared. RESULTS: Of 595 intensive care unit admissions, 212 (35.6{\%}) and 81 (13.6{\%}) were identified as severe sepsis and DIC, respectively, using laboratory data. The sensitivity of procedure-based methods for identifying severe sepsis was 64.2{\%}, and the specificity was 65.3{\%}. Two diagnosis-based methods -the Angus and Martin algorithms- exhibited sensitivities of 21.7{\%} and 14.6{\%} and specificities of 98.7{\%} and 99.5{\%}, respectively, for severe sepsis. For DIC, the sensitivity of procedure-based methods was 55.6{\%}, and the specificity was 67.1{\%}, and the sensitivity and specificity of diagnosis-based methods were 35.8{\%} and 98.2{\%}, respectively. CONCLUSIONS: Procedure-based methods were more sensitive and less specific than diagnosis-based methods in identifying severe sepsis and DIC. Procedure records could improve disease identification in administrative databases.},
author = {Yamana, Hayato and Horiguchi, Hiromasa and Fushimi, Kiyohide and Yasunaga, Hideo},
doi = {10.2188/jea.JE20150286},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Yamana et al. - 2016 - Comparison of Procedure-Based and Diagnosis-Based Identifications of Severe Sepsis and Disseminated Intravascular.pdf:pdf},
issn = {0917-5040},
journal = {Journal of Epidemiology},
keywords = {administrative data,disseminated intravascular coagulation,procedure record,severe sepsis},
number = {10},
pages = {1--8},
pmid = {27064132},
title = {{Comparison of Procedure-Based and Diagnosis-Based Identifications of Severe Sepsis and Disseminated Intravascular Coagulation in Administrative Data}},
volume = {26},
year = {2016}
}
@article{Saitta1995,
author = {Cortes, Corinna and Vapnik, Vladimir},
doi = {10.1007/BF00994018},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/SVM/Cortes{\_}Vapnik{\_}1995{\_}ML{\_}Support-Vector Networks.pdf:pdf},
isbn = {0885-6125},
issn = {0885-6125},
journal = {Machine Learning},
keywords = {efficient learning algorithms,neural networks,pattern recognition,polynomial classifiers,radial basis function classifiers},
month = {sep},
number = {3},
pages = {273--297},
pmid = {19549084},
title = {{Support-vector networks}},
volume = {20},
year = {1995}
}
@article{Friedman2002,
author = {Friedman, Jerome H.},
doi = {10.1016/S0167-9473(01)00065-2},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Tree/Friedman{\_}2002{\_}CSDA{\_}Stochastic gradient boosting.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics {\&} Data Analysis},
month = {feb},
number = {4},
pages = {367--378},
title = {{Stochastic gradient boosting}},
volume = {38},
year = {2002}
}
@article{Hastie1994,
abstract = {Fisher's linear discriminant analysis is a valuable tool for multigroup classification. With a large number of predictors, one can find a reduced number of discriminant coordinate functions that are "optimal" for separating the groups. With two such functions, one can produce a classification map that partitions the reduced space into regions that are identified with group membership, and the decision boundaries are linear. This article is about richer nonlinear classification schemes. Linear discriminant analysis is equivalent to multiresponse linear regression using optimal scorings to represent the groups. In this paper, we obtain nonparametric versions of discriminant analysis by replacing linear regression by any nonparametric regression method. In this way, any multiresponse regression technique (such as MARS or neural networks) can be postprocessed to improve its classification performanc},
author = {Hastie, Trevor and Tibshirani, Robert and Buja, Andreas},
doi = {10.1080/01621459.1994.10476866},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/LDAs/Hastie{\_}Tibshirani{\_}Buja{\_}1994{\_}JASA{\_}Flexible discriminant analysis by optimal scoring.pdf:pdf},
isbn = {0162-1459},
issn = {0162-1459},
journal = {Journal of the American Statistical Association},
keywords = {MARS,classification,discriminant analysis,nonparametric regression},
month = {dec},
number = {428},
pages = {1255--1270},
title = {{Flexible Discriminant Analysis by Optimal Scoring}},
volume = {89},
year = {1994}
}
@article{Schermerhorn2015,
abstract = {BackgroundRandomized trials and observational studies have shown that perioperative morbidity and mortality are lower with endovascular repair of abdominal aortic aneurysm than with open repair, but the survival benefit is not sustained. In addition, concerns have been raised about the long-term risk of aneurysm rupture or the need for reintervention after endovascular repair. MethodsWe assessed perioperative and long-term survival, reinterventions, and complications after endovascular repair as compared with open repair of abdominal aortic aneurysm in propensity-score–matched cohorts of Medicare beneficiaries who underwent repair during the period from 2001 through 2008 and were followed through 2009. ResultsWe identified 39,966 matched pairs of patients who had undergone either open repair or endovascular repair. The overall perioperative mortality was 1.6{\%} with endovascular repair versus 5.2{\%} with open repair (P{\textless}0.001). From 2001 through 2008, perioperative mortality decreased by 0.8 percentage points ...},
author = {Schermerhorn, Marc L. and Buck, Dominique B. and O'Malley, A. James and Curran, Thomas and McCallum, John C. and Darling, Jeremy and Landon, Bruce E.},
doi = {10.1056/NEJMoa1405778},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Schermerhorn et al. - 2015 - Long-Term Outcomes of Abdominal Aortic Aneurysm in the Medicare Population.pdf:pdf},
isbn = {1533-4406 (Electronic)$\backslash$r0028-4793 (Linking)},
issn = {0028-4793},
journal = {New England Journal of Medicine},
number = {4},
pages = {328--338},
pmid = {26200979},
title = {{Long-Term Outcomes of Abdominal Aortic Aneurysm in the Medicare Population}},
volume = {373},
year = {2015}
}
@article{Kimura2010,
author = {Kimura, Shinya and Sato, Toshihiko and Ikeda, Shunya and Noda, Mitsuhiko and Nakayama, Takeo},
doi = {10.2188/jea.JE20090066},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/Kimura{\_}et{\_}al{\_}2010{\_}JEpi{\_}Development of a Database of Health Insurance Claims- Standardization of Disease Classifications and Anonymous Record Linkage.pdf:pdf},
issn = {1349-9092},
journal = {Journal of Epidemiology},
keywords = {administrative data,database,health insurance claims,health services research,record linkage},
month = {sep},
number = {5},
pages = {413--419},
title = {{Development of a Database of Health Insurance Claims: Standardization of Disease Classifications and Anonymous Record Linkage}},
volume = {20},
year = {2010}
}
@article{Hastie1996,
abstract = {Nearest neighbour classification expects the class conditional$\backslash$nprobabilities to be locally constant, and suffers from bias in high$\backslash$ndimensions. We propose a locally adaptive form of nearest neighbour$\backslash$nclassification to try to ameliorate this curse of dimensionality. We use$\backslash$na local linear discriminant analysis to estimate an effective metric for$\backslash$ncomputing neighbourhoods. We determine the local decision boundaries$\backslash$nfrom centroid information, and then shrink neighbourhoods in directions$\backslash$northogonal to these local decision boundaries, and elongate them$\backslash$nparallel to the boundaries. Thereafter, any neighbourhood-based$\backslash$nclassifier can be employed, using the modified neighbourhoods. The$\backslash$nposterior probabilities tend to be more homogeneous in the modified$\backslash$nneighbourhoods. We also propose a method for global dimension reduction,$\backslash$nthat combines local dimension information. In a number of examples, the$\backslash$nmethods demonstrate the potential for substantial improvements over$\backslash$nnearest neighbour classification},
author = {Hastie, Trevor and Tibshirani, Robert},
doi = {10.1109/34.506411},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Nearest{\_}Neighbors/Hastie{\_}TIbshirani{\_}1996{\_}IEEE{\_}Discriminant Adaptive Nearest Neighbor Classification.pdf:pdf},
isbn = {0162-8828},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
keywords = {Classification,Curse of dimensionality,Linear discriminant analysis,Nearest neighbors},
month = {jun},
number = {6},
pages = {607--616},
title = {{Discriminant adaptive nearest neighbor classification}},
volume = {18},
year = {1996}
}
@article{VanWalraven2016,
abstract = {Background Migraine is a common and important source of pain and disability in society. Accurately identifying such people using routinely collected health data would be beneficial for health services research. Objective Externally validate a previously published method to identify migraineurs using health administrative data; and determine if a better model can be derived using data-mining techniques. Methods Migraine status was determined for Ontarians participating in a population-based, cross-sectional survey. Consenting participants were linked to population-based health administrative data to identify age, sex, and coded diagnoses. Discrimination and calibration measures were used to appraise the models. A de novo technique we term “double threshold analysis” was used to determine optimal lower and upper expected probabilities to identify migraine status in the newly derived model. Results A total of 1,01,114 people (mean age 46 years, 46{\%} male) were included in the study, of which 11,314 (11.2{\%}) had migraines. Using data-driven parameter estimates, the previous model to identify migraineurs had adequate discrimination (c-statistic 0.707 [95{\%} CI 0.701–0.712]) and calibration (Hosmer–Lemeshow [H–L] statistic 20.8). A new model that included diagnostic code scores for physician visits, emergency visits, and hospitalizations with nonlinear terms for age and interactions significantly improved the model (c-statistic 0.724 [0.716–0.733], 16.4). Categorizing all people with a predicted migraine probability less than 10{\%} or greater than 90{\%} as without and having the disease, respectively, resulted in a sensitivity of 3.1{\%}, a specificity of 99.96{\%}, and a positive predictive value of 81.0{\%} while capturing 57.0{\%} of the cohort and 29.3{\%} of migraineurs. Conclusion A previously derived model to identify migraineurs was improved using data-mining techniques permitting accurate cohort identification using routinely collected health administrative data.},
author = {van Walraven, Carl and Colman, Ian},
doi = {10.1016/j.jclinepi.2015.09.007},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML{\_}CBA/Walraven{\_}Colman{\_}2016{\_}JCE{\_}Migraineurs were reliably identified using administrative data.pdf:pdf},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Administrative data,Claims signature model,Data mining,Migraine,Multivariable logistic regression,Validation},
month = {mar},
pages = {68--75},
pmid = {26404461},
publisher = {Elsevier Inc},
title = {{Migraineurs were reliably identified using administrative data}},
volume = {71},
year = {2016}
}
@article{Hasan2016,
abstract = {We present R package mnlogit for training multinomial logistic regression models, particularly those involving a large number of classes and features. Compared to existing software, mnlogit offers speedups of 10x-50x for modestly sized problems and more than 100x for larger problems. Running mnlogit in parallel mode on a multicore machine gives an additional 2x-4x speedup on up to 8 processor cores. Computational efficiency is achieved by drastically speeding up calculation of the log-likelihood function's Hessian matrix by exploiting structure in matrices that arise in intermediate calculations.},
author = {Hasan, Asad and Wang, Zhiyu and Mahani, Alireza S.},
doi = {10.18637/jss.v075.i03},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Hasan, Wang, Mahani - 2016 - Fast Estimation of Multinomial Logit Models R Package mnlogit.pdf:pdf},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {discrete choice,econo-,large scale,logistic regression,multinomial logit,parallel},
number = {3},
title = {{Fast Estimation of Multinomial Logit Models: R Package mnlogit}},
volume = {75},
year = {2016}
}
@article{Shevade2003,
author = {Shevade, S. K. and Keerthi, S. S.},
doi = {10.1093/bioinformatics/btg308},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Shrinkage/Shevade{\_}Keerthi{\_}2003{\_}Bioinformatics{\_}A simple and efficient algorithm for gene selection using sparse logistic regression.pdf:pdf},
issn = {1367-4803},
journal = {Bioinformatics},
month = {nov},
number = {17},
pages = {2246--2253},
title = {{A simple and efficient algorithm for gene selection using sparse logistic regression}},
volume = {19},
year = {2003}
}
@article{Quan2009,
abstract = {We validated the accuracy of case definitions for hypertension derived from administrative data across time periods (year 2001 versus 2004) and geographic regions using physician charts. Physician charts were randomly selected in rural and urban areas from Alberta and British Columbia, Canada, during years 2001 and 2004. Physician charts were linked with administrative data through unique personal health number. We reviewed charts of approximately 50 randomly selected patients {\textgreater}35 years of age from each clinic within 48 urban and 16 rural family physician clinics to identify physician diagnoses of hypertension during the years 2001 and 2004. The validity indices were estimated for diagnosed hypertension using 3 years of administrative data for the 8 case-definition combinations. Of the 3,362 patient charts reviewed, the prevalence of hypertension ranged from 18.8{\%} to 33.3{\%}, depending on the year and region studied. The administrative data hypertension definition of "2 claims within 2 years or 1 hospitalization" had the highest validity relative to the other definitions evaluated (sensitivity 75{\%}, specificity 94{\%}, positive predictive value 81{\%}, negative predictive value 92{\%}, and kappa 0.71). After adjustment for age, sex, and comorbid conditions, the sensitivities between regions, years, and provinces were not significantly different, but the positive predictive value varied slightly across geographic regions. These results provide evidence that administrative data can be used as a relatively valid source of data to define cases of hypertension for surveillance and research purposes.},
author = {Quan, Hude and Khan, Nadia and Hemmelgarn, Brenda R. and Tu, Karen and Chen, Guanmin and Campbell, Norm and Hill, Michael D. and Ghali, William A. and McAlister, Finlay A.},
doi = {10.1161/HYPERTENSIONAHA.109.139279},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Quan et al. - 2009 - Validation of a case definition to define hypertension using administrative data.pdf:pdf},
isbn = {1524-4563 (Electronic)$\backslash$r0194-911X (Linking)},
issn = {0194911X},
journal = {Hypertension},
keywords = {Administrative data,Health information,Hypertension,International Disease Classification,Surveillance},
number = {6},
pages = {1423--1428},
pmid = {19858407},
title = {{Validation of a case definition to define hypertension using administrative data}},
volume = {54},
year = {2009}
}
@article{Segal2004,
author = {Segal, Mark R.},
journal = {UCSF: Center for Bioinformatics and Molecular Biostatistics},
title = {{Machine learning benchmarks and random forest regression}},
year = {2004}
}
@techreport{Fix1951,
author = {Fix, Evelyn and Hodges, J.L.},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Nearest{\_}Neighbors/Fix{\_}Hodges{\_}1951{\_}TR{\_}Discriminatory Analysis - Nonparametric Discrimination- Consistency Properties.pdf:pdf},
institution = {U.S. Air Force, School of Aviation Medicine, Randolph Field},
title = {{Discriminatory Analysis - Nonparametric Discrimination: Consistency Properties}},
year = {1951}
}
@article{Khera2018,
abstract = {In 2009, the US government mandated that all health care institutions and practitioners covered by the Health Insurance Portability and Accountability Act must tran-sition to a new set of codes for transmitting information about patients' conditions and treatments using the International Statistical Classification of Diseases and Related Health Problems, Tenth Revision (ICD-10). The transition, which was delayed twice, to these codes from the International Classification of Diseases, Ninth Revi-sion (ICD-9) took effect in October 2015. 1 Data from health care encounters coded as ICD-10 are just now becoming available and this change to the ICD-10 has rendered ICD-coded data more challenging to interpret and use. Hospitals use ICD-coded data to track and charac-terize patients, record treatments, monitor outcomes, and seek financial reimbursement from health insur-ance programs, whereas insurance programs use these data to track the health of those they insure. The Cen-ters for Medicare {\&} Medicaid Services (CMS), in particu-lar, uses these data to identify specific patient cohorts for its quality measures and to define case mix at hos-pitals, which are relevant for assessments of care qual-ity and determining penalties for suboptimal care. 2 More-over, several CMS programs, including Medicare Advantage and Value-Based Purchasing, exclusively rely on ICD-coded data to define the expected care needs of patients and to influence payments.},
author = {Khera, Rohan and Dorsey, Karen B. and Krumholz, Harlan M.},
doi = {10.1001/jama.2018.6823},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/Editorial/Khera{\_}et{\_}al{\_}2018{\_}JAMA{\_}Transition to the ICD-10 in the United States.pdf:pdf},
issn = {0098-7484},
journal = {JAMA},
month = {jul},
number = {2},
pages = {133},
title = {{Transition to the ICD-10 in the United States}},
volume = {320},
year = {2018}
}
@misc{Stevenson2018,
author = {Stevenson, Mark and Nunes, Telmo and Heuer, Cord and Marshall, Jonathon and Sanchez, Javier and Thornton, Ron and Reiczigel, Jeno and Robison-Cox, Jim and Sebastiani, Paola and Solymos, Peter and Yoshida, Kazuki and Jones, Geoff and Pirikahu, Sarah and Firestone, Simon and Kyle, Ryan and Popp, Johann and Jay, Mathew},
title = {{epiR: Tools for the Analysis of Epidemiological Data}},
url = {https://cran.r-project.org/package=epiR},
year = {2018}
}
@article{Tessier-Sherman2013,
abstract = {BACKGROUND: The practice of using medical service claims in epidemiologic research on hypertension is becoming increasingly common, and several published studies have attempted to validate the diagnostic data contained therein. However, very few of those studies have had the benefit of using actual measured blood pressure as the gold standard. The goal of this study is to assess the validity of claims data in identifying hypertension cases and thereby clarify the benefits and limitations of using those data in studies of chronic disease etiology.$\backslash$n$\backslash$nMETHODS: Disease status was assigned to 19,150 employees at a U.S. manufacturing company where regular physical examinations are performed. We compared the presence of hypertension in the occupational medical charts against diagnoses obtained from administrative claims data.$\backslash$n$\backslash$nRESULTS: After adjusting for potential confounders, those with measured blood pressure indicating stage 1 hypertension were 3.69 times more likely to have a claim than normotensives (95{\%} CI: 3.12, 4.38) and those indicating stage 2 hypertension were 7.70 times more likely to have a claim than normotensives (95{\%} CI: 6.36, 9.35). Comparing measured blood pressure values identified in the medical charts to the algorithms for diagnosis of hypertension from the claims data yielded sensitivity values of 43-61{\%} and specificity values of 86-94{\%}.$\backslash$n$\backslash$nCONCLUSIONS: The medical service claims data were found to be highly specific, while sensitivity values varied by claims algorithm suggesting the possibility of under-ascertainment. Our analysis further demonstrates that such under-ascertainment is strongly skewed toward those cases that would be considered clinically borderline or mild.},
author = {Tessier-Sherman, Baylah and Galusha, Deron and Taiwo, Oyebode a and Cantley, Linda and Slade, Martin D and Kirsche, Sharon R and Cullen, Mark R},
doi = {10.1186/1471-2458-13-51},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Tessier-Sherman et al. - 2013 - Further validation that claims data are a useful tool for epidemiologic research on hypertension.pdf:pdf},
issn = {1471-2458},
journal = {BMC public health},
keywords = {Blood Pressure,Confounding Factors (Epidemiology),Databases,Epidemiologic Research Design,Factual,Humans,Hypertension,Hypertension: diagnosis,Industry,Insurance Claim Review,Insurance Claim Review: standards,United States},
pages = {51},
pmid = {23331960},
title = {{Further validation that claims data are a useful tool for epidemiologic research on hypertension.}},
volume = {13},
year = {2013}
}
@article{Fawcett2006,
abstract = {Receiver operating characteristics (ROC) graphs are useful for organizing classifiers and visualizing their performance. ROC graphs are commonly used in medical decision making, and in recent years have been used increasingly in machine learning and data mining research. Although ROC graphs are apparently simple, there are some common misconceptions and pitfalls when using them in practice. The purpose of this article is to serve as an introduction to ROC graphs and as a guide for using them in research. {\textcopyright} 2005 Elsevier B.V. All rights reserved.},
author = {Fawcett, Tom},
doi = {10.1016/j.patrec.2005.10.010},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ROC/Fawcett{\_}2006{\_}PatRecogLet{\_}An introduction to ROC analysis.pdf:pdf},
isbn = {01678655},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Classifier evaluation,Evaluation metrics,ROC analysis},
month = {jun},
number = {8},
pages = {861--874},
pmid = {9820922},
title = {{An introduction to ROC analysis}},
volume = {27},
year = {2006}
}
@article{Zou2005,
abstract = {Summary. We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p≫n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.},
author = {Zou, Hui and Hastie, Trevor},
doi = {10.1111/j.1467-9868.2005.00503.x},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Shrinkage/Zou{\_}Hastie{\_}2005{\_}JRSSB{\_}Regularization and variable selection via the elastic net.pdf:pdf},
journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
keywords = {Grouping effect,LARS algorithm,Lasso,Penalization,Variable selection,p≫n problem},
number = {2},
pages = {301--320},
title = {{Regularization and variable selection via the elastic net}},
volume = {67},
year = {2005}
}
@article{Einav2015,
author = {Einav, Liran and Finkelstein, Amy and Schrimpf, Paul},
doi = {10.1093/qje/qjv005.Advance},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Einav, Finkelstein, Schrimpf - 2015 - The Response of Drug Expenditure to Contract Design in Medicare Part D.pdf:pdf},
journal = {Quarterly Journal of Economics},
keywords = {and many seminar participants,and nber,contract design,department,department of economics,edu,einav,finkelstein,for helpful discussions,from the nia,health care,health insurance,leinav,maria polyakova,medicare,moral hazard,provided extraordinary research assistance,r01 ag032449,ray kluender,stanford,stanford university,we gratefully acknowledge support,we thank jason abaluck},
number = {2},
pages = {841--899},
title = {{The Response of Drug Expenditure to Contract Design in Medicare Part D}},
volume = {130},
year = {2015}
}
@article{Gran2009,
abstract = {Although sample size calculations have become an important element in the design of research projects, such methods for studies involving current status data are scarce. Here, we propose a method for calculating power and sample size for studies using current status data. This method is based on a Weibull survival model for a two-group comparison. The Weibull model allows the investigator to specify a group difference in terms of a hazards ratio or a failure time ratio. We consider exponential, Weibull and uniformly distributed censoring distributions. We base our power calculations on a parametric approach with the Wald test because it is easy for medical investigators to conceptualize and specify the required input variables. As expected, studies with current status data have substantially less power than studies with the usual right-censored failure time data. Our simulation results demonstrate the merits of these proposed power calculations.},
author = {Gran, J.M. and Wasmuth, L. and Amundsen, E.J. and Lindqvist, B.H. and Aalen, O.O.},
doi = {10.1002/sim},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Gran et al. - 2009 - Growth rates in epidemic models application to a model for HIVAIDS progression.pdf:pdf},
isbn = {2007090091480},
issn = {02776715},
journal = {Statistics in medicine},
keywords = {ces-d,conditional maximum likelihood,fixed effects,generalized linear mixed model,hausman test,linear mixed model,random effects,robust,variance},
number = {June 2007},
pages = {221--239},
pmid = {19455509},
title = {{Growth rates in epidemic models: application to a model for HIV/AIDS progression}},
volume = {28},
year = {2009}
}
@article{Hastie1986,
author = {Hastie, Trevor and Tibshirani, Robert},
doi = {10.2307/2245459},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/GAM/Hastie{\_}Tibshirani{\_}1986{\_}StatSci{\_}Generalized Additive Models.pdf:pdf},
journal = {Statistical Science},
number = {3},
pages = {297--310},
title = {{Generalized Additive Models}},
volume = {1},
year = {1986}
}
@misc{WHOb,
author = {WHO},
title = {{WHOCC - ATC/DDD Index}},
url = {https://www.whocc.no/atc{\_}ddd{\_}index/},
urldate = {2018-10-10}
}
@article{Bullano2006,
abstract = {OBJECTIVE: This study evaluated the accuracy of 2 administrative claims-based selection rules to identify patients with hypertension (HTN) using medical records as the gold standard.$\backslash$n$\backslash$nRESEARCH DESIGN: The claims database consisted of inpatient, outpatient, pharmacy, and eligibility claims for members of a single insurance company from January 2000 through March 2003. Medical records were abstracted for 258 matched patient pairs selected by Rule A (at least 1 HTN-related International Classification of Diseases, 9th Revision [ICD-9] claim) and 138 pairs selected by Rule B (at least 1 HTN-related ICD-9 and at least 1 HTN prescription claim) from 31 provider sites. Sensitivity and specificity of the 2 selection rules were computed using medical chart review as the gold standard for a diagnosis of HTN.$\backslash$n$\backslash$nSUBJECTS: Of patients selected by Rule A, chart review identified 281 patients with and 235 patients without HTN. Of patients selected by Rule B, chart review identified 172 patients with and 104 patients without HTN.$\backslash$n$\backslash$nRESULTS: The sensitivity and specificity was 70.8{\%} and 74.9{\%} for Rule A and 76.2{\%} and 93.3{\%} for Rule B. The kappa score was 0.45 for Rule A and 0.65 for Rule B.$\backslash$n$\backslash$nCONCLUSION: To identify patients with HTN, a selection rule using both a diagnosis and prescription claim has greater sensitivity and specificity than a rule using a diagnosis claim only.},
author = {Bullano, Michael F. and Kamat, Siddhesh and Willey, Vincent J. and Barlas, Suna and Watson, Douglas J. and Brenneman, Susan K.},
doi = {10.1097/01.mlr.0000207482.02503.55},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Bullano et al. - 2006 - Agreement between administrative claims and the medical record in identifying patients with a diagnosis of hyper.pdf:pdf},
isbn = {0025-7079 (Print)},
issn = {0025-7079},
journal = {Medical Care},
keywords = {Databases,Factual,Female,Guidelines as Topic,Humans,Hypertension,Hypertension: diagnosis,Insurance,Insurance Claim Review,Insurance Claim Review: standards,Male,Medical Records,Mid-Atlantic Region,Middle Aged,Pharmaceutical Services,Pharmaceutical Services: statistics {\&} n,Reproducibility of Results},
month = {may},
number = {5},
pages = {486--490},
pmid = {16641668},
title = {{Agreement Between Administrative Claims and the Medical Record in Identifying Patients With a Diagnosis of Hypertension}},
volume = {44},
year = {2006}
}
@incollection{Rumelhart1986a,
address = {Cambridge},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition},
chapter = {8},
editor = {Rumelhart, David E. and McClelland, James L.},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Neural{\_}Networks/Rumelhart{\_}Hinton{\_}Williams{\_}1986{\_}Learning internal representations by error propagation.pdf:pdf},
pages = {318--362},
publisher = {MIT Press},
title = {{Learning Internal Representations by Error Propagation}},
volume = {1},
year = {1986}
}
@article{Abrahamowicz2007,
abstract = {Objective: To evaluate alternative approaches to correct for bias due to inaccurate diagnostic criteria in database studies of associations. Study Design and Settings: A simulation study of a hypothetical cohort of 10,000 subjects selected based on database-derived diagnostic criteria with positive predictive value (PPV) of either 53{\%} or 80{\%}. Analyses focus on the putative association between a drug and the time to a negative outcome. The association is confounded for "false positive" subjects, where the drug acts as a marker for unobserved frailty. First, we estimate the conventional multivariable Cox's Model 1. We then assume having in-depth evaluation of a fraction of subjects, which permits estimating the probabilities of having the disease for all subjects in the cohort. Alternative correction methods use the estimated probability as a confounder (Model 2), a modifier of the drug effect (Model 3), or an importance weight (Model 4). Results: With a PPV of 53{\%}, Models 1 and 2 induced about 50{\%} underestimation bias for the drug effect. Interaction-based Model 3 yielded the least biased estimates (25{\%} bias), whereas weighting by probability (Model 4) resulted in slightly more biased (33{\%}), but more stable estimates. Conclusion: Proposed methods help reducing bias due to sample contamination. ?? 2007 Elsevier Inc. All rights reserved.},
author = {Abrahamowicz, Michal and Xiao, Yongling and Ionescu-Ittu, Raluca and Lacaille, Diane},
doi = {10.1016/j.jclinepi.2006.07.016},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Abrahamowicz et al. - 2007 - Simulations showed that validation of database-derived diagnostic criteria based on a small subsample reduc.pdf:pdf},
isbn = {0895-4356 (Print)$\backslash$r0895-4356 (Linking)},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Accuracy,Administrative databases,Bias correction,Simulations,Statistical modeling,Survival analysis},
number = {6},
pages = {600--609},
pmid = {17493519},
title = {{Simulations showed that validation of database-derived diagnostic criteria based on a small subsample reduced bias}},
volume = {60},
year = {2007}
}
@book{TheJapanDiabetesSocietyEds.2016,
address = {Tokyo},
author = {{The Japan Diabetes Society (Eds.)}},
publisher = {The Japan Diabetes Society},
title = {{Guidelines for the management of Diabetes 2016 (in Japanese)}},
year = {2016}
}
@article{Tibshirani1996,
abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
author = {Tibshirani, Robert},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Shrinkage/Tibshirani{\_}1996{\_}JRSSB{\_}Regression Shrinkage and Selection via the Lasso.pdf:pdf},
journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
keywords = {quadratic programming,regression,shrinkage,subset selection},
number = {1},
pages = {267--288},
title = {{Regression Shrinkage and Selection via the Lasso}},
volume = {58},
year = {1996}
}
@misc{Hastie2018,
author = {Hastie, Trevor},
title = {{gam: Generalized Additive Models}},
url = {https://cran.r-project.org/package=gam},
year = {2018}
}
@article{Yamana2017,
author = {Yamana, Hayato and Moriwaki, Mutsuko and Horiguchi, Hiromasa and Kodan, Mariko and Fushimi, Kiyohide and Yasunaga, Hide},
doi = {10.1016/j.je.2016.09.009},
file = {:Users/harakonan/Dropbox/Research/Collaboration/COI/Validation Check/Reference/Yamana{\_}etal{\_}2017{\_}JE.pdf:pdf},
issn = {09175040},
journal = {Journal of Epidemiology},
pages = {1--7},
publisher = {Elsevier Ltd},
title = {{Validity of diagnoses, procedures, and laboratory data in Japanese administrative data}},
year = {2017}
}
@article{Perkins2006,
abstract = {The use of biomarkers is of ever-increasing importance in clinical diagnosis of disease. In practice, a cutpoint is required for dichotomizing naturally continuous biomarker levels to distinguish persons at risk of disease from those who are not. Two methods commonly used for establishing the "optimal" cutpoint are the point on the receiver operating characteristic curve closest to (0,1) and the Youden index, J. Both have sound intuitive interpretations--the point closest to perfect differentiation and the point farthest from none, respectively--and are generalizable to weighted sensitivity and specificity. Under the same weighting of sensitivity and specificity, these two methods identify the same cutpoint as "optimal" in certain situations but different cutpoints in others. In this paper, the authors examine situations in which the two criteria agree or disagree and show that J is the only "optimal" cutpoint for given weighting with respect to overall misclassification rates. A data-driven example is used to clarify and demonstrate the magnitude of the differences. The authors also demonstrate a slight alteration in the (0,1) criterion that retains its intuitive meaning while resulting in consistent agreement with J. In conclusion, the authors urge that great care be taken when establishing a biomarker cutpoint for clinical use.},
author = {Perkins, Neil J. and Schisterman, Enrique F.},
doi = {10.1093/aje/kwj063},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ROC/Perkins{\_}Schisterman{\_}2006{\_}AJE{\_}The Inconsistency of ‘‘Optimal'' Cutpoints Obtained using Two Criteria based on the Receiver Operating Characteristic Curve.pdf:pdf},
isbn = {0002-9262 (Print)$\backslash$r0002-9262 (Linking)},
issn = {00029262},
journal = {American Journal of Epidemiology},
keywords = {Area under curve,Biological markers,Cutpoints,Data interpretation,Epidemiologic methods,Perkins2006,ROC curve,Statistics,Youden index,statistical},
mendeley-tags = {Perkins2006},
number = {7},
pages = {670--675},
pmid = {16410346},
title = {{The inconsistency of "optimal" cutpoints obtained using two criteria based on the receiver operating characteristic curve}},
volume = {163},
year = {2006}
}
@book{Greene2012,
address = {Upper Saddle River},
author = {Greene, William H},
edition = {7th},
publisher = {Prentice Hall},
title = {{Econometric analysis}},
year = {2012}
}
@article{Abaluck2016,
abstract = {We study choice over prescription insurance plans by the elderly using government administrative data to evaluate how these choices evolve over time. We find large " foregone savings " from not choosing the lowest cost plan that has grown over time. We develop a struc-tural framework to decompose the changes in " foregone welfare " from inconsistent choices into choice set changes and choice func-tion changes from a fixed choice set. We find that foregone welfare increases over time due primarily to changes in plan characteristics such as premiums and out-of-pocket costs; we estimate little learning at either the individual or cohort level. (JEL G22, H51, I13, I18, J14) The past five years has seen a sea change in the way that publicly provided health insurance benefits are delivered to the US population. From the introduction of the Medicare and Medicaid programs in 1965, expansions in public health insurance entitlements came through the extension of these monopoly government-run insur-ance plans. But beginning with the introduction of the Medicare Part D Prescription drug program in 2006, and continuing through the exchanges that are at the center of the Affordable Care Act (ACA) that was passed in 2010, the United States has been moving to a different model: insurance exchanges where the publicly insured choose from a host of subsidized private insurance options. This privatization of the delivery of public insurance raises a host of interesting policy and research questions. Primary among these is the ability of individuals to make consistent choices across a potentially large array of choices with meaningful differences. In Abaluck and Gruber (2011), we explored this issue in the context of the Part D program. We considered in particular whether elders appeared to be properly weighing the premium and out-of-pocket spending implications of their plan choices. We concluded that they were not, with the typical elder able to save},
author = {Abaluck, Jason and Gruber, Jonathan},
doi = {10.1257/aer.20130778},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Abaluck, Gruber - 2016 - Evolving choice inconsistencies in choice of prescription drug insurance.pdf:pdf},
issn = {00028282},
journal = {American Economic Review},
number = {8},
pages = {2145--2184},
title = {{Evolving choice inconsistencies in choice of prescription drug insurance}},
volume = {106},
year = {2016}
}
@article{Virnig2001,
abstract = {Electronically available administrative data are increasingly used by public health researchers and planners. The validity of the data source has been established, and its strengths and weaknesses relative to data abstracted from medical records and obtained via survey are documented. Administrative data are available from a variety of state, federal, and private sources and can, in many cases, be combined. As a tool for planning and surveillance, administrative data show great promise: They contain consistent elements, are available in a timely manner, and provide information about large numbers of individuals. Because they are available in an electronic format, they are relatively inexpensive to obtain and use. In the United States, however, there is no administrative data set covering the entire population. Although Medicare provides health care for an estimated 96{\%} of the elderly, age 65 years and older, there is no comparable source for those under 65.},
author = {Virnig, Beth A and McBean, Marshall},
doi = {10.1146/annurev.publhealth.22.1.213},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Virnig, McBean - 2001 - Administrative data for public health surveillance and planning.pdf:pdf},
issn = {0163-7525},
journal = {Annual Review of Public Health},
keywords = {abstracted from medical,administrative data are increasingly,and its strengths and,claims data,data source has been,diagnosis,es-,geographic variation,insurance data,planners,public health researchers and,s abstract electronically available,tablished,the validity of the,used by,validity,weaknesses relative to data},
month = {may},
number = {1},
pages = {213--230},
pmid = {11274519},
title = {{Administrative Data for Public Health Surveillance and Planning}},
volume = {22},
year = {2001}
}
@article{Greenberg2002,
abstract = {Background. Continuity of care (COC) has often been viewed as a crucial indicator of treatment quality for patients with severe psychiatric or addictive disorders. However, the relationship between COC and clinical outcomes has received little empirical evaluation. Research Design. This study used hierarchical linear modeling to examine the relationship between six indicators of COC and seven outcome measures addressing symptoms, substance abuse, and social functioning. Subjects. Patient interviews were conducted with 1576 veterans 3 months after their discharge from one of 22 residential work therapy programs for the treatment of severe substance abuse. Results. Few significant relationships were found between COC and outcome measures in analyses conducted at both the client and program level and fewer than half of these show better outcomes with greater COC. When a Bonferroni corrected P level of P {\textless}0.0012 was used, none of the relationships were statistically significant. Conclusion. Although there were significant relationships between outcomes and measures of services received during residential treatment, postdischarge COC does not seem to be related to improved outcomes, at least when examined following long term intensive residential treatment. Thus, our results are specific to the context of aftercare following long-term residential rehabilitation and indicate that the value of standard performance measures may vary by treatment context. CR - Copyright {\&}{\#}169; 2002 Lippincott Williams {\&} Wilkins},
author = {Greenberg, Greg A and Rosenheck, Robert A and Seibyl, Catherine L},
doi = {10.2307/3767611},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Greenberg, Rosenheck, Seibyl - 2002 - Continuity of Care and Clinical Effectiveness Outcomes Following Residential Treatment for Severe.pdf:pdf},
isbn = {0025-7079},
issn = {00257079},
journal = {Medical Care},
keywords = {2002,40,ad-,administrative databases have become,an ac-,claims data,comorbidity,iv-26,iv-35,med care,medicare,ministrative data,seer,supplement},
number = {3},
pages = {246--259},
pmid = {12187165},
title = {{Continuity of Care and Clinical Effectiveness: Outcomes Following Residential Treatment for Severe Substance Abuse}},
volume = {40},
year = {2002}
}
@article{Hastie1995,
abstract = {Fisher's linear discriminant analysis (LDA) is a popular data-analytic tool for studying the relationship between a set of predictors and a categorical response. In this paper we describe a penalized version of LDA. It is designed for situations in which there are many highly correlated predictors, such as those obtained by discretizing a function, or the grey-scale values of the pixels in a series of images. In cases such as these it is natural, efficient and sometimes essential to impose a spatial smoothness constraint on the coefficients, both for improved prediction performance and interpretability. We cast the classification problem into a regression framework via optimal scoring. Using this, our proposal facilitates the use of any penalized regression technique in the classification setting. The technique is illustrated with examples in speech recognition and handwritten character recognition.},
author = {Hastie, Trevor and Buja, Andreas and Tibshirani, Robert},
doi = {10.2307/2242400},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/LDAs/Hastie{\_}Buja{\_}Tibshirani{\_}1995{\_}AnnStat{\_}Penalized discriminant analysisi.pdf:pdf},
issn = {00905364},
journal = {The Annals of Statistics},
month = {may},
number = {1},
pages = {73--102},
title = {{Penalized Discriminant Analysis}},
volume = {23},
year = {1995}
}
@article{Sands1999,
abstract = {Although most surgical site infections (SSIs) occur after hospital discharge, there is no efficient way to identify them. The utility of automated claims and electronic medical record data for this purpose was assessed in a cohort of 4086 nonobstetric procedures following which 96 postdischarge SSIs occurred. Coded diagnoses, tests, and treatments were assessed by use of recursive partitioning, with 10-fold cross-validation, and logistic regression with bootstrap resampling. Specific codes and combinations of codes identified a subset of 2{\%} of all procedures among which 74{\%} of SSIs had occurred. Accepting a specificity of 92{\%} improved the sensitivity from 74{\%} to 92{\%}. Use of only hospital discharge diagnosis codes plus pharmacy dispensing data had sensitivity of 77{\%} and specificity of 94{\%}. All of these performance characteristics were better than questionnaire responses from patients or surgeons. Thus, information routinely collected by health care systems can be the basis of an efficient, largely passive, surveillance system for postdischarge SSIs.},
author = {Sands, Kenneth and Vineyard, Gordon and Livingston, James and Christiansen, Cindy and Platt, Richard},
doi = {10.1086/314586},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML{\_}CBA/Sands{\_}et{\_}al{\_}1999{\_}JInfectDisease{\_}Efficient Identification of Postdischarge Surgical Site Infections- Use of Automated Pharmacy Dispensing Information, Administrative Data, and Medical Record Information.pdf:pdf},
isbn = {0022-1899 (Print)$\backslash$n0022-1899 (Linking)},
issn = {0022-1899},
journal = {The Journal of Infectious Diseases},
month = {feb},
number = {2},
pages = {434--441},
pmid = {9878028},
title = {{Efficient Identification of Postdischarge Surgical Site Infections: Use of Automated Pharmacy Dispensing Information, Administrative Data, and Medical Record Information}},
volume = {179},
year = {1999}
}
@article{Taylor2009,
abstract = {Our study estimates the sensitivity and specificity of Medicare claims to identify clinically-diagnosed dementia, and documents how errors in dementia assessment affect dementia cost estimates. We compared Medicare claims from 1993-2005 to clinical dementia assessments carried out in 2001-2003 for the Aging Demographics and Memory Study (ADAMS) cohort (n = 758) of the Health and Retirement Study. The sensitivity and specificity of Medicare claims was 0.85 and 0.89 for dementia (0.64 and 0.95 for AD). Persons with dementia cost the Medicare program (in 2003) {\$}7,135 more than controls (P {\textless} 0.001) when using claims to identify dementia, compared to {\$}5,684 more when using ADAMS (P {\textless} 0.001). Using Medicare claims to identify dementia results in a 110{\%} increase in costs for those with dementia as compared to a 68{\%} increase when using ADAMS to identify disease, net of other variables. Persons with false positive Medicare claims notations of dementia were the most expensive group of subjects ({\$}11,294 versus {\$}4,065, for true negatives P {\textless} 0.001). Medicare claims overcount the true prevalence of dementia, but there are both false positive and negative assessments of disease. The use of Medicare claims to identify dementia results in an overstatement of the increase in Medicare costs that are due to dementia.},
author = {Taylor, Donald H. and {\O}stbye, Truls and Langa, Kenneth M. and Weir, David and Plassman, Brenda L.},
doi = {10.3233/JAD-2009-1099},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Taylor et al. - 2009 - The accuracy of medicare claims as an epidemiological tool The case of dementia revisited.pdf:pdf},
isbn = {1387-2877},
issn = {13872877},
journal = {Journal of Alzheimer's Disease},
keywords = {Dementia costs,Medicare,Sensitivity,Specificity},
number = {4},
pages = {807--815},
pmid = {19542620},
title = {{The accuracy of medicare claims as an epidemiological tool: The case of dementia revisited}},
volume = {17},
year = {2009}
}
@article{Fan2008,
author = {Rong-En, Fan and Kai-Wei, Chang and Cho-Jui, Hsieh and Xiang-Rui, Wang and Chih-Jen, Lin},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/SVM/Fan{\_}et{\_}al{\_}2008{\_}JMLR{\_}LIBLINEAR- A Library for Large Linear Classification.pdf:pdf},
journal = {Journal of Machine Learning Research},
keywords = {large-scale linear classification,logistic regression,machine learning,open,source,support vector machines},
pages = {1871 -- 1874},
title = {{LIBLINEAR: A Library for Large Linear Classification}},
volume = {9},
year = {2008}
}
@article{Layton2017,
abstract = {Importance Testosterone initiation increased substantially in the United States from 2000 to 2013, especially among men without clear indications. Direct-to-consumer advertising (DTCA) also increased during this time. Objective To investigate associations between televised DTCA and testosterone testing and initiation in the United States. Design, Setting, and Population Ecologic study conducted in designated market areas (DMAs) in the United States. Monthly testosterone advertising ratings were linked to DMA-level testosterone use data from 2009-2013 derived from commercial insurance claims. Associations between DTCA and testosterone testing, initiation, and initiation without recent baseline tests were estimated using Poisson generalized estimating equations. Exposures Monthly Nielsen ratings for testosterone DTCA in the 75 largest DMAs. Main Outcomes and Measures (1) Rates of new serum testosterone testing; (2) rates of testosterone initiation (in-office injection, surgical implant, or pharmacy dispensing) for all testosterone products combined and for specific brands; and (3) rates of testosterone initiation without recent serum testosterone testing. Results Of 17 228 599 commercially insured men in the 75 DMAs, 1 007 990 (mean age, 49.6 [SD, 11.5] years) had new serum testosterone tests and 283 317 (mean age, 51.8 [SD, 11.3] years) initiated testosterone treatment. Advertising intensity varied by geographic region and time, with the highest intensity seen in the southeastern United States and with months ranging from no ad exposures to a mean of 13.6 exposures per household. Nonbranded advertisements were common prior to 2012, with branded advertisements becoming more common during and after 2012. Each household advertisement exposure was associated with a monthly increase in rates of new testosterone testing (rate ratio [RR], 1.006; 95{\%} CI, 1.004-1.008), initiation (RR, 1.007; 95{\%} CI, 1.004-1.010), and initiation without a recent test (RR, 1.008; 95{\%} CI, 1.002-1.013). Mean absolute rate increases were 0.14 tests (95{\%} CI, 0.09-0.19), 0.05 new initiations (95{\%} CI, 0.03-0.08), and 0.02 initiations without a recent test (95{\%} CI, 0.01-0.03) per 10 000 men for each monthly ad exposure over the entire period. Conclusions and Relevance Among US men residing in the 75 designated market areas, regional exposure to televised direct-to-consumer advertising was associated with greater testosterone testing, new initiation, and initiation without recent testing.},
author = {Layton, J. Bradley and Kim, Yoonsang and Alexander, G. Caleb and Emery, Sherry L.},
doi = {10.1001/jama.2016.21041},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Layton et al. - 2017 - Association Between Direct-to-Consumer Advertising and Testosterone Testing and Initiation in the United States,.pdf:pdf},
issn = {1538-3598},
journal = {JAMA},
number = {11},
pages = {1159--1166},
pmid = {28324090},
title = {{Association Between Direct-to-Consumer Advertising and Testosterone Testing and Initiation in the United States, 2009-2013.}},
volume = {317},
year = {2017}
}
@article{Nattinger2004,
abstract = {OBJECTIVE: To develop and validate a clinically informed algorithm that uses solely Medicare claims to identify, with a high positive predictive value, incident breast cancer cases. DATA SOURCE: Population-based Surveillance, Epidemiology, and End Results (SEER) Tumor Registry data linked to Medicare claims, and Medicare claims from a 5 percent random sample of beneficiaries in SEER areas. STUDY DESIGN: An algorithm was developed using claims from 1995 breast cancer patients from the SEER-Medicare database, as well as 1995 claims from Medicare control subjects. The algorithm was validated on claims from breast cancer subjects and controls from 1994. The algorithm development process used both clinical insight and logistic regression methods. DATA EXTRACTION: Training set: Claims from 7,700 SEER-Medicare breast cancer subjects diagnosed in 1995, and 124,884 controls. Validation set: Claims from 7,607 SEER-Medicare breast cancer subjects diagnosed in 1994, and 120,317 controls. PRINCIPAL FINDINGS: A four-step prediction algorithm was developed and validated. It has a positive predictive value of 89 to 93 percent, and a sensitivity of 80 percent for identifying incident breast cancer. The sensitivity is 82-87 percent for stage I or II, and lower for other stages. The sensitivity is 82-83 percent for women who underwent either breast-conserving surgery or mastectomy, and is similar across geographic sites. A cohort identified with this algorithm will have 89-93 percent incident breast cancer cases, 1.5-6 percent cancer-free cases, and 4-5 percent prevalent breast cancer cases. CONCLUSIONS: This algorithm has better performance characteristics than previously proposed algorithms. The ability to examine national patterns of breast cancer care using Medicare claims data would open new avenues for the assessment of quality of care.},
author = {Nattinger, Ann B. and Laud, Purushottam W. and Bajorunaite, Ruta and Sparapani, Rodney A. and Freeman, Jean L.},
doi = {10.1111/j.1475-6773.2004.00315.x},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Nattinger et al. - 2004 - An algorithm for the use of Medicare claims data to identify women with incident breast cancer.pdf:pdf},
isbn = {0017-9124 (Print)$\backslash$r0017-9124 (Linking)},
issn = {0017-9124},
journal = {Health Services Research},
keywords = {breast neoplasm,incidence,registries,sensitivity and specificity},
month = {dec},
number = {6},
pages = {1733--1750},
pmid = {15533184},
title = {{An Algorithm for the Use of Medicare Claims Data to Identify Women with Incident Breast Cancer}},
volume = {39},
year = {2004}
}
@article{Steinwart2005,
abstract = {It is shown that various classifiers that are based on minimization of a regularized risk are universally consistent, i.e., they can asymptotically learn in every classification task. The role of the loss functions used in these algorithms is considered in detail. As an application of our general framework, several types of support vector machines (SVMs) as well as regularization networks are treated. Our methods combine techniques from stochastics, approximation theory, and functional analysis},
author = {Steinwart, Ingo},
doi = {10.1109/TIT.2004.839514},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/SVM/Steinwart{\_}2005{\_}IEEE{\_}Consistency of Support Vector Machines and Other Regularized Kernel Classifiers.pdf:pdf},
issn = {0018-9448},
journal = {IEEE Transactions on Information Theory},
keywords = {Computational learning theory,Kernel methods,Pattern recognition,Regularization,Support vector machines (SVMs),Universal consistency},
month = {jan},
number = {1},
pages = {128--142},
title = {{Consistency of Support Vector Machines and Other Regularized Kernel Classifiers}},
volume = {51},
year = {2005}
}
@article{Gold2007,
abstract = {OBJECTIVE: To test the validity of three published algorithms designed to identify incident breast cancer cases using recent inpatient, outpatient, and physician insurance claims data. DATA: The Surveillance, Epidemiology, and End Results (SEER) registry data linked with Medicare physician, hospital, and outpatient claims data for breast cancer cases diagnosed from 1995 to 1998 and a 5 percent control sample of Medicare beneficiaries in SEER areas. STUDY DESIGN: We evaluate the sensitivity and specificity of three algorithms applied to new data compared with original reported results. Algorithms use health insurance diagnosis and procedure claims codes to classify breast cancer cases, with SEER as the reference standard. We compare algorithms by age, stage, race, and SEER region, and explore via logistic regression whether adding demographic variables improves algorithm performance. PRINCIPAL FINDINGS: The sensitivity of two of three algorithms is significantly lower when applied to newer data, compared with sensitivity calculated during algorithm development (59 and 77.4 percent versus 90 and 80.2 percent, p{\textless}.00001). Sensitivity decreases as age increases, and false negative rates are higher for cases with in situ, metastatic, and unknown stage disease compared with localized or regional breast cancer. Substantial variation also exists by SEER registry. There was potential for improvement in algorithm performance when adding age, region, and race to an indicator variable for whether the algorithm determined a subject to be a breast cancer case (p{\textless}.00001). CONCLUSIONS: Differential sensitivity of the algorithms by SEER region and age likely reflects variation in practice patterns, because the algorithms rely on administrative procedure codes. Depending on the algorithm, 3-5 percent of subjects overall are misclassified in 1998. Misclassification disproportionately affects older women and those diagnosed with in situ, metastatic, or unknown-stage disease. Algorithms should be applied cautiously to insurance claims databases to assess health care utilization outside SEER-Medicare populations because of uneven misclassification of subgroups that may be understudied already.},
author = {Gold, Heather T. and Do, Huong T.},
doi = {10.1111/j.1475-6773.2007.00705.x},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Gold, Do - 2007 - Evaluation of three algorithms to identify incident breast cancer in medicare claims data.pdf:pdf},
isbn = {0017-9124 (Print)$\backslash$n0017-9124 (Linking)},
issn = {00179124},
journal = {Health Services Research},
keywords = {Algorithm validation,Breast neoplasm,Incidence,Medicare,Registries},
number = {5},
pages = {2056--2069},
pmid = {17850533},
title = {{Evaluation of three algorithms to identify incident breast cancer in medicare claims data}},
volume = {42},
year = {2007}
}
@article{Bradley1997,
abstract = {In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six “real world” medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for “single number” evaluation of machine learning algorithms.},
author = {Bradley, Andrew P.},
doi = {10.1016/S0031-3203(96)00142-2},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ROC/Bradley{\_}1997{\_}PatRec{\_}The use of the area under the roc curve in the evaluation of machine learning algorithms.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
month = {jul},
number = {7},
pages = {1145--1159},
pmid = {17569110},
title = {{The use of the area under the ROC curve in the evaluation of machine learning algorithms}},
volume = {30},
year = {1997}
}
@article{Freeman2000,
abstract = {This study developed and evaluated a method for ascertaining a newly diagnosed breast cancer case using multiple sources of data from the Medicare claims system. Predictors of an incident case were operationally defined as codes for breast cancer-related diagnoses and procedures from hospital inpatient, hospital outpatient, and physician claims. The optimal combination of predictors was then determined from a logistic regression model using 1992 data from the linked SEER registries-Medicare claims data base and a sample of noncancer controls drawn from the SEER areas. While the ROC curve demonstrates that the model can produce levels of sensitivity and specificity above 90{\%}, the positive predictive value is comparatively low (67-70{\%}). This low predictive value is largely the result of the model's limitation in distinguishing recurrent and secondary malignancies from incident cases and possibly from the model identifying true incident cases not identified by SEER. Nevertheless, the logistic regression approach is a useful method for ascertaining incident cases because it allows for greater flexibility in changing the performance characteristics by selecting different cut-points depending on the application (e.g., high sensitivity for registry validation, high specificity for outcomes research). It also allows us to make specific adjustments to population based estimates of breast cancer incidence with claims.},
author = {Freeman, Jean L. and Zhang, Dong and Freeman, Daniel H. and Goodwin, James S.},
doi = {10.1016/S0895-4356(99)00173-0},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Freeman et al. - 2000 - An approach to identifying incident breast cancer cases using Medicare claims data.pdf:pdf},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Aged,Breast Neoplasms,Breast Neoplasms: diagnosis,Breast Neoplasms: epidemiology,Breast Neoplasms: surgery,Female,Humans,Incidence,Insurance Claim Reporting,Logistic Models,Medicare,ROC Curve,SEER Program,United States,United States: epidemiology},
month = {jun},
number = {6},
pages = {605--614},
pmid = {10880779},
title = {{An approach to identifying incident breast cancer cases using Medicare claims data}},
volume = {53},
year = {2000}
}
@article{Wilchesky2004,
abstract = {Objectives Few studies have attempted to validate the diagnostic information contained within medical service claims data, and only a small proportion of these have attempted to do so using the medical chart as a gold standard. The goal of this study is to determine the sensitivity and specificity of medical services claims diagnoses for surveillance of 14 drug disease contraindications used in drug utilization review, the Charlson comorbidity index and the Johns Hopkins Adjusted Care Group Case-Mix profile (ADGs). Study design and setting Diagnoses were abstracted from the medical charts of 14,980 patients, and were used as the "gold standard," against which diagnoses obtained from the administrative database for the same patients were compared. Results Conditions associated with drug disease contraindications with the exception of hypertension and chronic obstructive pulmonary disease (COPD) showed a specificity of 90{\%} or higher. Sensitivity of claims data was substantially lower, with glaucoma, hypertension, and diabetes being the most sensitive conditions at 76, 69, and 64{\%}, respectively. Each of the 18 disease conditions contained in the Charlson comorbidity index showed high specificity, but sensitivity was more variable among conditions as well as by coding definitions. Although ADG specificity was also high, the vast majority of ADGs had sensitivities of less than 60{\%}. Conclusion The administrative data was found to have diagnoses and conditions that were highly specific but that vary greatly by condition in terms of sensitivity. To appropriately obtain diagnostic profiles, it is recommended that data pertaining to all physician billings be used. ?? 2004 Elsevier Inc. All rights reserved.},
author = {Wilchesky, Machelle and Tamblyn, Robyn M. and Huang, Allen},
doi = {10.1016/S0895-4356(03)00246-4},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Wilchesky, Tamblyn, Huang - 2004 - Validation of diagnostic codes within medical services claims.pdf:pdf},
isbn = {0895-4356 (Print)$\backslash$n0895-4356 (Linking)},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Diagnositc codes,Medical Services claims},
number = {2},
pages = {131--141},
pmid = {15125622},
title = {{Validation of diagnostic codes within medical services claims}},
volume = {57},
year = {2004}
}
@article{Belloni2014,
author = {Belloni, Alexandre and Chernozhukov, Victor and Hansen, Christian},
doi = {10.1093/restud/rdt044},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/HDS/Belloni{\_}Chernozhukov{\_}Hansen{\_}2014{\_}RES{\_}Inference on Treatment Effects after Selection among High-Dimensional Controls.pdf:pdf},
issn = {0034-6527},
journal = {The Review of Economic Studies},
keywords = {average treatment effects,high-dimensional-sparse regression,imperfect model selection,inference under,lasso,model selection,orthogonality of estimating equations,parameters,partially linear model,treatment effects,uniformly valid inference after,with respect to nuisance},
month = {apr},
number = {2},
pages = {608--650},
title = {{Inference on Treatment Effects after Selection among High-Dimensional Controls}},
volume = {81},
year = {2014}
}
@article{Friedman1991,
abstract = {A new method is presented for flexible regression modeling of high dimensional data. The model takes the form of an expansion in product spline basis functions, where the number of basis functions as well as the parameters associated with each one (product degree and knot locations) are automatically determined by the data. This procedure is motivated by the recursive partitioning approach to regression and shares its attractive properties. Unlike recursive partitioning, however, this method produces continuous models with continuous derivatives. It has more power and flexibility to model relationships that are nearly additive or involve interactions in at most a few variables. In addition, the model can be represented in a form that separately identifies the additive contributions and those associated with the different multivariable interactions.},
author = {Friedman, Jerome H.},
doi = {10.1214/aos/1176347963},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/LDAs/Freidman{\_}1991{\_}AnnStat{\_}Multivariate adaptive regression splines.pdf:pdf},
isbn = {00905364},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = {mar},
number = {1},
pages = {1--67},
pmid = {19892930},
title = {{Multivariate Adaptive Regression Splines}},
volume = {19},
year = {1991}
}
@article{Tu2011,
abstract = {Objective: With the increasing use of electronic medical records (EMRs) comes the potential to efficiently evaluate and improve quality of care. We set out to determine if diabetics could be accurately identified using structured data contained within an EMR. Study Design and Setting: We used a 5{\%} random sample of adult patients (969 patients) within a convenience sample of 17 primary care physicians using Practices Solutions EMR in Ontario. A reference standard of diabetes status was manually confirmed by reviewing each patient's record. Accuracy for identifying people with diabetes was assessed using various combinations of laboratory tests and prescriptions. EMR data was also compared with administrative data. Results: A rule of one elevated blood sugar or a prescription for an antidiabetic medication had a 83.1{\%} sensitivity, 98.2{\%} specificity, 80.0{\%} positive predictive value (PPV) and 98.5{\%} negative predictive value (NPV) compared with the reference standard of diabetes status. Conclusion: We found that the use of structured data within an EMR could be used to identify patients with diabetes. Our results have positive implications for policy makers, researchers, and clinicians as they develop registries of diabetic patients to examine quality of care using EMR data. ?? 2011 Elsevier Inc. All rights reserved.},
author = {Tu, Karen and Manuel, Doug and Lam, Kelvin and Kavanagh, Doug and Mitiku, Tezeta F. and Guo, Helen},
doi = {10.1016/j.jclinepi.2010.04.007},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Tu et al. - 2011 - Diabetics can be identified in an electronic medical record using laboratory tests and prescriptions.pdf:pdf},
isbn = {1878-5921},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Administrative data,Diabetes,Electronic medical records},
number = {4},
pages = {431--435},
pmid = {20638237},
publisher = {Elsevier Inc},
title = {{Diabetics can be identified in an electronic medical record using laboratory tests and prescriptions}},
volume = {64},
year = {2011}
}
@article{Upadhyaya2017,
abstract = {Abstract Objective To develop and validate a phenotyping algorithm for the identification of patients with type 1 and type 2 diabetes mellitus (DM) preoperatively using routinely available clinical data from electronic health records. Patients and Methods We used first-order logic rules (if-then-else rules) to imply the presence or absence of DM types 1 and 2. The "if" clause of each rule is a conjunction of logical and, or predicates that provides evidence toward or against the presence of DM. The rule includes International Classification of Diseases, Ninth Revision, Clinical Modification diagnostic codes, outpatient prescription information, laboratory values, and positive annotation of DM in patients' clinical notes. This study was conducted from March 2, 2015, through February 10, 2016. The performance of our rule-based approach and similar approaches proposed by other institutions was evaluated with a reference standard created by an expert reviewer and implemented for routine clinical care at an academic medical center. Results A total of 4208 surgical patients (mean age, 52 years; males, 48{\%}) were analyzed to develop the phenotyping algorithm. Expert review identified 685 patients (16.28{\%} of the full cohort) as having DM. Our proposed method identified 684 patients (16.25{\%}) as having DM. The algorithm performed well—99.70{\%} sensitivity, 99.97{\%} specificity—and compared favorably with previous approaches. Conclusion Among patients undergoing surgery, determination of DM can be made with high accuracy using simple, computationally efficient rules. Knowledge of patients' DM status before surgery may alter physicians' care plan and reduce postsurgical complications. Nevertheless, future efforts are necessary to determine the effect of first-order logic rules on clinical processes and patient outcomes.},
author = {Upadhyaya, Sudhi G. and Murphree, Dennis H. and Ngufor, Che G. and Knight, Alison M. and Cronk, Daniel J. and Cima, Robert R. and Curry, Timothy B. and Pathak, Jyotishman and Carter, Rickey E. and Kor, Daryl J.},
doi = {10.1016/j.mayocpiqo.2017.04.005},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/Phenotyping/Upadhyaya{\_}et{\_}al{\_}2017{\_}Mayo{\_}Automated Diabetes Case Identification Using Electronic Health Record Data at a Tertiary Care Facility.pdf:pdf},
isbn = {978-973-8468-80-1},
issn = {25424548},
journal = {Mayo Clinic Proceedings: Innovations, Quality {\&} Outcomes},
month = {jul},
number = {1},
pages = {100--110},
publisher = {Mayo Foundation for Medical Education and Research},
title = {{Automated Diabetes Case Identification Using Electronic Health Record Data at a Tertiary Care Facility}},
volume = {1},
year = {2017}
}
@article{Okamoto2010,
abstract = {BACKGROUND: An ideal classification should have maximum intercategory variance and minimal intracategory variance. Health insurance claims typically include multiple diagnoses and are classified into different disease categories by choosing principal diagnoses. The accuracy of classification based on principal diagnoses was evaluated by comparing intercategory and intracategory variance of per-claim costs and the trend in accuracy was reviewed.$\backslash$n$\backslash$nMETHODS: Means and standard deviations of log-transformed per-claim costs were estimated from outpatient claims data from the National Health Insurance Medical Benefit Surveys of 1995 to 2007, a period during which only the ICD10 classification was applied. Intercategory and intracategory variances were calculated for each of 38 mutually exclusive disease categories and the percentage of intercategory variance to overall variance was calculated to assess the trend in accuracy of classification.$\backslash$n$\backslash$nRESULTS: A declining trend in the percentage of intercategory variance was observed: from 19.5{\%} in 1995 to 10{\%} in 2007. This suggests that there was a decline in the accuracy of disease classification in discriminating per-claim costs for different disease categories. The declining trend temporarily reversed in 2002, when hospitals and clinics were directed to assign the principal diagnosis. However, this reversal was only temporary and the declining trend appears to be consistent.$\backslash$n$\backslash$nCONCLUSIONS: Classification of health insurance claims based on principal diagnoses is becoming progressively less accurate in discriminating per-claim costs. Researchers who estimate disease-specific health care costs using health insurance claims must therefore proceed with caution.},
author = {Okamoto, Etsuji},
doi = {JST.JSTAGE/jea/JE20090044 [pii]},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Okamoto - 2010 - Declining accuracy in disease classification on health insurance claims should we reconsider classification by principa.pdf:pdf},
isbn = {1349-9092 (Electronic)$\backslash$n0917-5040 (Linking)},
issn = {1349-9092},
journal = {Journal of epidemiology / Japan Epidemiological Association},
keywords = {Disease,Disease: classification,Health Care Costs,Health Care Costs: statistics {\&} numerical data,Health Care Surveys,Humans,Insurance Claim Reporting,Insurance Claim Reporting: standards,Insurance Claim Reporting: trends,Japan,Medical Records,National Health Programs,National Health Programs: economics,National Health Programs: statistics {\&} numerical d},
number = {2},
pages = {166--75},
pmid = {20065616},
title = {{Declining accuracy in disease classification on health insurance claims: should we reconsider classification by principal diagnosis?}},
volume = {20},
year = {2010}
}
@article{Newton2013,
abstract = {One of the most used dimensions for comparing human values at the cultural level is that of individualism-collectivism. It was originally proposed by Hofstede (1984), and continues to be employed in current theoretical models such as those of Triandis (1995) and Schwartz (1994). Although the Hofstede and Schwartz models have been compared in previous studies, there is little data that permits an evaluation of their explanatory potential with respect to macro-social and macro-economic variables. Furthermore, even when there is evidence to the relation of the individualism-collectivism dimension with others, such as power distance, autonomy and conservation, they are not usually treated in the same study. In this sense, our work compares these two models in relation to the values of individualism-collectivism. With this goal in mind, the same 20 countries that have scores in Hofstede and Schwartz's studies on these dimensions are compared in relation to a group of macro-social (birth rate, human development, illiteracy rate, etc.) and macro-economic (gross national product, rate of agricultural activity, rate of inflation, etc.) variables. Results show that the Hofstede model is better explained by macro-economic variables while the Schwartz model is better accounted for by macro-social variables.},
author = {Newton, K. M. and Peissig, P. L. and Kho, A. N. and Bielinski, S. J. and Berg, R. L. and Choudhary, V. and Basford, M. and Chute, C. G. and Kullo, I. J. and Li, R. and Pacheco, J. A. and Rasmussen, L. V. and Spangler, L. and Denny, J. C.},
doi = {10.1136/amiajnl-2012-000896},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/Phenotyping/Newton{\_}et{\_}al{\_}2013{\_}JAMIA{\_}Validation of electronic medical record-based phenotyping algorithms results and lessons learned from the eMERGE network.pdf:pdf},
isbn = {0214-9915 U6 - ctx{\_}ver=Z39.88-2004{\&}ctx{\_}enc=info{\%}3Aofi{\%}2Fenc{\%}3AUTF-8{\&}rfr{\_}id=info:sid/summon.serialssolutions.com{\&}rft{\_}val{\_}fmt=info:ofi/fmt:kev:mtx:journal{\&}rft.genre=article{\&}rft.atitle=Hofstede+and+Schwartz{\%}27s+models+for+classifying+individualism+at+the+cultural+level{\%}3A+their+relation+to+macro-social+and+macro-economic+variables{\&}rft.jtitle=PSICOTHEMA{\&}rft.au=Gouveia{\%}2C+VV{\&}rft.au=Ros{\%}2C+M{\&}rft.date=2000{\&}rft.pub=COLEGIO+OFICIAL+DE+PSICOLOGOS+DE+ASTURIAS{\&}rft.issn=0214-9915{\&}rft.eissn=1886-144X{\&}rft.volu},
issn = {1067-5027},
journal = {Journal of the American Medical Informatics Association},
month = {jun},
pages = {e147--e154},
pmid = {23531748},
title = {{Validation of electronic medical record-based phenotyping algorithms: results and lessons learned from the eMERGE network}},
volume = {20},
year = {2013}
}
@techreport{Lecun1989,
author = {LeCun, Y.},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Neural{\_}Networks/LeCun{\_}1989{\_}TR{\_}Generalization and Network Design Strategies.pdf:pdf},
institution = {Department of Computer Science, Univ. of Toronto},
title = {{Generalization and Network Design Strategies}},
year = {1989}
}
@article{Klabunde2000,
abstract = {Important comorbidities recorded on outpatient claims in administrative datasets may be missed in analyses when only inpatient care is considered. Using the comorbid conditions identified by Charlson and colleagues, we developed a comorbidity index that incorporates the diagnostic and procedure data contained in Medicare physician (Part B) claims. In the national cohorts of elderly prostate (n = 28,868) and breast cancer (n = 14,943) patients assessed in this study, less than 10{\%} of patients had comorbid conditions identified when only Medicare hospital (Part A) claims were examined. By incorporating physician claims, the proportion of patients with comorbid conditions increased to 25{\%}. The new physician claims comorbidity index significantly contributes to models of 2-year noncancer mortality and treatment received in both patient cohorts. We demonstrate the utility of a disease-specific index using an alternative method of construction employing study-specific weights. The physician claims index can be used in conjunction with a comorbidity index derived from inpatient hospital claims, or employed as a stand-alone measure. Copyright ?? 2000 Elsevier Science Inc.},
author = {Klabunde, Carrie N. and Potosky, Arnold L. and Legler, Julie M. and Warren, Joan L.},
doi = {10.1016/S0895-4356(00)00256-0},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Klabunde et al. - 2000 - Development of a comorbidity index using physician claims data.pdf:pdf},
isbn = {0895-4356 (Print)$\backslash$r0895-4356 (Linking)},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Administrative data,Breast cancer,Claims data,Comorbidity,Medicare,Prostate cancer},
number = {12},
pages = {1258--1267},
pmid = {11146273},
title = {{Development of a comorbidity index using physician claims data}},
volume = {53},
year = {2000}
}
@article{Hinton2006,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
doi = {10.1162/neco.2006.18.7.1527},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Neural{\_}Networks/Hinton{\_}Osindero{\_}Teh{\_}2006{\_}NeuralComp{\_}A Fast Learning Algorithm for Deep Belief Nets.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
month = {jul},
number = {7},
pages = {1527--1554},
pmid = {16764513},
title = {{A Fast Learning Algorithm for Deep Belief Nets}},
volume = {18},
year = {2006}
}
@article{Mitchell1994,
abstract = {Medicare claims databases have several advantages for use in constructing episodes of care for outcomes research. They are population-based, relatively inexpensive to obtain, include large numbers of cases, and can be used for long-term follow-up. However, the sheer size of these claims databases, along with their primarily administrative (as opposed to clinical) nature, requires that researchers take special care in using them. The 10 PORTs using Medicare claims provided information on their approach to several key issues in working with these data, including: 1) identifying the index cases or patient cohorts to be studied; 2) defining the length of the episode; and 3) measuring outcomes. This paper reports the experience and knowledge gained by these PORTs in using these claims to create and analyze episodes of care},
author = {Mitchell, Janet B. and Bubolz, Thomas and Paul, John E. and Pashos, Chris L. and Escarce, Jos{\'{e}} J. and Muhlbaier, Lawrence H. and Wiesman, John M. and Young, Wanda W. and Epstein, Robert S. and Javitt, Jonathan C.},
doi = {10.1097/00005650-199407001-00004},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/Others/Mitchell{\_}et{\_}al{\_}1994{\_}MedCare{\_}Using Medicare Claims for Outcomes Research.pdf:pdf},
isbn = {0025-7079},
issn = {0025-7079},
journal = {Medical Care},
month = {jul},
number = {7},
pages = {JS38--JS51},
pmid = {8028412},
title = {{Using Medicare Claims for Outcomes Research}},
volume = {32},
year = {1994}
}
@article{Friedman2010,
abstract = {We develop fast algorithms for estimation of generalized linear models with convex penalties. The models include linear regression, two-class logistic regression, and multinomial regression problems while the penalties include ℓ(1) (the lasso), ℓ(2) (ridge regression) and mixtures of the two (the elastic net). The algorithms use cyclical coordinate descent, computed along a regularization path. The methods can handle large problems and can also deal efficiently with sparse features. In comparative timings we find that the new algorithms are considerably faster than competing methods.},
author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Rob},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Shrinkage/Friedman{\_}Hastie{\_}Tibshirani{\_}2010{\_}JStatSoft{\_}Regularization Paths for Generalized Linear Models via Coordinate Descent.pdf:pdf},
isbn = {9781584886822},
issn = {1548-7660},
journal = {Journal of statistical software},
keywords = {elastic net,lasso,logistic regression},
month = {may},
number = {1},
pages = {1--22},
pmid = {20808728},
title = {{Regularization Paths for Generalized Linear Models via Coordinate Descent.}},
volume = {33},
year = {2010}
}
@article{Robinson1997,
abstract = {OBJECTIVES A cardiovascular health survey of a representative sample of the adult population of Manitoba, Canada was combined with the provincial health insurance claims database to determine the accuracy of survey questions in detecting cases of diabetes, hypertension, ischemic heart disease, stroke, and hypercholesterolemia. METHODS Of 2,792 subjects in the survey, 97.7{\%} were linked successfully using a scrambled personal health insurance number. Hospital and physician claims were extracted for these individuals for the 3-year period before the survey. RESULTS The authors found no benefits to using restrictive criteria for entrance into the study (ie, requiring more than one diagnosis to define a case). Using additional years of data increased agreement between data sources. Kappa values indicated high levels of agreement between administrative data and self-reports for diabetes (0.72) and hypertension (0.59); kappa values were approximately 0.4 for the other conditions. Using administrative data as the "gold standard," specificity was generally very high, although cases with hypertension and hypercholesterolemia (diagnosed primarily by laboratory or physical measurement) were associated with a lower specificity than the other conditions. Sensitivity varied markedly and was lowest for "other heart disease" and "stroke". For diabetes and hypertension, inclusion criteria calling for more than one diagnosis reduced the accuracy of case identification, whereas increasing the number of years of data increased accuracy of identification. For diabetes and hypertension, self-reports were fairly accurate in detecting "true" past history of the illness based on physician diagnosis recorded on insurance claims. CONCLUSIONS This study demonstrates the feasibility of linking a large health survey with administrative data and the validity of self-reports in estimating the prevalence of chronic diseases, especially diabetes and hypertension. A linked data set offers unusual opportunities for epidemiologic and health services research in a defined population.},
author = {Robinson, J. Reneé and Young, T. Kue and Roos, Leslie L. and Gelskey, Dale E.},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Robinson et al. - 1997 - Estimating the burden of disease. Comparing administrative data and self-reports.pdf:pdf},
issn = {0025-7079},
journal = {Medical care},
month = {sep},
number = {9},
pages = {932--947},
pmid = {9298082},
title = {{Estimating the burden of disease. Comparing administrative data and self-reports.}},
volume = {35},
year = {1997}
}
@article{Hebert1997,
abstract = {The objective of this study was to develop and validate a method for identifying Medicare beneficiaries with diabetes by using Medicare claims data. We used self-reports of diabetes status from participants in the Medicare Current Beneficiary Survey to determine disease status, and then we examined these participants' Medicare claims. Using self-reported diabetes status as the "gold standard," we determined the sensitivity, specificity, and reliability of claims data in identifying beneficiaries with diabetes. We found that to construct a method that is adequately sensitive ({\textgreater} or = 70{\%}), highly specific ({\textgreater} or = 97.5{\%}), and reliable (kappa {\textgreater} or = 0.80), researchers must combine information from different types of Medicare claims files, use 2 years of data to identify cases, and require at least 2 diagnoses of diabetes among claims involving ambulatory care. Since these criteria are met by more than one method, the choice of method should be governed by the goals of the research as well as more practical concerns.},
author = {Hebert, Paul L. and Geiss, Linda S. and Tierney, Edward F. and Engelgau, Michael M. and Yawn, Barbara P. and McBean, A. Marshall},
doi = {10.1177/106286069901400607},
issn = {1062-8606},
journal = {American Journal of Medical Quality},
month = {nov},
number = {6},
pages = {270--277},
pmid = {10624032},
title = {{Identifying Persons with Diabetes Using Medicare Claims Data}},
volume = {14},
year = {1999}
}
@techreport{Friedman2003,
author = {Friedman, Jerome H. and Popescu, Bogdan E.},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Tree/Friedman{\_}Popuscu{\_}2003{\_}TR{\_}Importance Sampled Learning Ensembles.pdf:pdf},
institution = {Department of Statistics, Stanford University},
keywords = {AdaBoost,MART,bagging,boosting,gradient boosting,learning ensembles,random forests},
pages = {1--32},
title = {{Importance Sampled Learning Ensembles}},
year = {2003}
}
@article{Chen2016,
abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
archivePrefix = {arXiv},
arxivId = {1603.02754},
author = {Chen, Tianqi and Guestrin, Carlos},
doi = {10.1145/2939672.2939785},
eprint = {1603.02754},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Tree/Chen{\_}Guestrin{\_}2016{\_}arXiv{\_}XGBoost- A Scalable Tree Boosting System.pdf:pdf},
isbn = {9781450342322},
issn = {0146-4833},
keywords = {large-scale machine learning},
month = {mar},
pages = {1--13},
pmid = {22942019},
title = {{XGBoost: A Scalable Tree Boosting System}},
year = {2016}
}
@article{Simel1991,
abstract = {Confidence intervals are important summary measures that provide useful information from clinical investigations, especially when comparing data from different populations or sites. Studies of a diagnostic test should include both point estimates and confidence intervals for the tests' sensitivity and specificity. Equally important measures of a test's efficiency are likelihood ratios at each test outcome level. We present a method for calculating likelihood ratio confidence intervals for tests that have positive or negative results, tests with non-positive/non-negative results, and tests reported on an ordinal outcome scale. In addition, we demonstrate a sample size estimation procedure for diagnostic test studies based on the desired likelihood ratio confidence interval. The renewed interest in confidence intervals in the medical literature is important, and should be extended to studies analyzing diagnostic tests. {\textcopyright} 1991.},
author = {Simel, David L. and Samsa, Gregory P. and Matchar, David B.},
doi = {10.1016/0895-4356(91)90128-V},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Simel, Samsa, Matchar - 1991 - Likelihood ratios with confidence Sample size estimation for diagnostic test studies.pdf:pdf},
isbn = {0895-4356 (Print)$\backslash$r0895-4356 (Linking)},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Confidence interval,Diagnostic test,Likelihood ratio,Sample size},
month = {jan},
number = {8},
pages = {763--770},
pmid = {1941027},
title = {{Likelihood ratios with confidence: Sample size estimation for diagnostic test studies}},
volume = {44},
year = {1991}
}
@article{DeLong1988,
author = {DeLong, Elizabeth R. and DeLong, David M. and Clarke-Pearson, Daniel L.},
doi = {10.2307/2531595},
issn = {0006341X},
journal = {Biometrics},
month = {sep},
number = {3},
pages = {837--845},
title = {{Comparing the Areas under Two or More Correlated Receiver Operating Characteristic Curves: A Nonparametric Approach}},
volume = {44},
year = {1988}
}
@article{Taylor2002,
abstract = {We linked Medicare claims data to information on 417 patients with a clinical diagnosis of Alzheimer's disease in the Consortium to Establish a Registry for Alzheimer's Disease (CERAD) to determine what proportion of them were identified as having Alzheimer's disease (AD) in Medicare claims records. Seventy-nine percent of these patients were identified as having AD using 5 years of claims data; 87{\%} were identified as demented when a broader set of ICD-9-CM codes was used. An Anderson-Gill counting process approach was used to model the "hazard" of patients being identified as having AD in Medicare claims data. CERAD patients with mild dementia were less likely to be identified in the claims data as having AD. Once identified in Medicare claims as having AD, patients were more likely to be so identified again. When using only the physician supplier and institutional outpatient files, approximately 75{\%} of CERAD patients were identified as having AD; hospital files used alone identified less than one-third (29{\%}) of the CERAD patients as having AD. The data indicate that at least 3 consecutive years of physician supplier and physician outpatient claim files should be used to identify Medicare beneficiaries with AD using Medicare claims. ?? 2002 Elsevier Science Inc. All rights reserved.},
author = {Taylor, Donald H. and Fillenbaum, Gerda G. and Ezell, Michael E.},
doi = {10.1016/S0895-4356(02)00452-3},
file = {:Users/harakonan/Library/Application Support/Mendeley Desktop/Downloaded/Taylor, Fillenbaum, Ezell - 2002 - The accuracy of medicare claims data in identifying Alzheimer's disease.pdf:pdf},
isbn = {0895-4356 (Print)},
issn = {08954356},
journal = {Journal of Clinical Epidemiology},
keywords = {Alzheimer's disease,Anderson-Gill counting process model,CERAD,Medicare},
number = {9},
pages = {929--937},
pmid = {12393082},
title = {{The accuracy of medicare claims data in identifying Alzheimer's disease}},
volume = {55},
year = {2002}
}
@article{Guyon2003,
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}}},
file = {:Users/harakonan/Dropbox/Research/Claims/JMDC/cba/Reference2/ML/Pre-processing/Guyon{\_}Elisseeff{\_}2003{\_}JMLR{\_}An Introduction to Variable and Feature Selection.pdf:pdf},
journal = {Journal of Machine Learning Research},
month = {oct},
pages = {1157--1182},
title = {{An Introduction to Variable and Feature Selection}},
volume = {3},
year = {2003}
}
